{"cells":[{"cell_type":"markdown","metadata":{"id":"8QqJbxx-_BU7"},"source":["#Install libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Fy54k2O-Q3N","outputId":"ff8f3ff7-6b10-4a91-e079-5f271cae7292"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n","    return self.__dep_map\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n","    raise AttributeError(attr)\n","AttributeError: _DistInfoDistribution__dep_map\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/cli/base_command.py\", line 167, in exc_logging_wrapper\n","    status = run_func(*args)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n","    return func(self, options, args)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/commands/install.py\", line 397, in run\n","    conflicts = self._determine_conflicts(to_install)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/commands/install.py\", line 529, in _determine_conflicts\n","    return check_install_conflicts(to_install)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n","    package_set, _ = create_package_set_from_installed()\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n","    dependencies = list(dist.iter_dependencies())\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 202, in iter_dependencies\n","    return self._dist.requires(extras)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n","    dm = self._dep_map\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n","    self.__dep_map = self._compute_dependencies()\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n","    reqs.extend(parse_requirements(req))\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n","    yield Requirement(line)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n","    super(Requirement, self).__init__(requirement_string)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n","    req = REQUIREMENT.parseString(requirement_string)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pyparsing/core.py\", line 1124, in parse_string\n","    loc, tokens = self._parse(instring, 0)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pyparsing/core.py\", line 3863, in parseImpl\n","    loc, exprtokens = e._parse(instring, loc, doActions)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pyparsing/core.py\", line 4091, in parseImpl\n","    return e._parse(\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pyparsing/core.py\", line 3841, in parseImpl\n","    loc, resultlist = self.exprs[0]._parse(\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pyparsing/core.py\", line 3841, in parseImpl\n","    loc, resultlist = self.exprs[0]._parse(\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_vendor/pyparsing/core.py\", line 855, in _parseNoCache\n","    ret_tokens = ParseResults(\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n","    return command.main(cmd_args)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n","    return self._main(args)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/cli/base_command.py\", line 221, in _main\n","    return run(options, args)\n","  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/cli/base_command.py\", line 204, in exc_logging_wrapper\n","    logger.critical(\"Operation cancelled by user\")\n","  File \"/usr/lib/python3.9/logging/__init__.py\", line 1493, in critical\n","    self._log(CRITICAL, msg, args, **kwargs)\n","  File \"/usr/lib/python3.9/logging/__init__.py\", line 1577, in _log\n","    fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n","  File \"/usr/lib/python3.9/logging/__init__.py\", line 1533, in findCaller\n","    filename = os.path.normcase(co.co_filename)\n","KeyboardInterrupt\n","^C\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.14.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.25.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (4.0.0)\n"]}],"source":["!pip install transformers\n","!pip install torch torchvision\n","!pip install pandas\n","!pip install numpy\n","!pip install datasets\n","!pip install pytorch_transformers\n","!pip install scikit-learn\n","!pip install matplotlib\n","!pip install seaborn\n","!pip install nltk"]},{"cell_type":"markdown","metadata":{"id":"J13iSqp__qMw"},"source":["Import the required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-shbksg4Wz2","executionInfo":{"status":"error","timestamp":1678640688751,"user_tz":-60,"elapsed":4182,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"colab":{"base_uri":"https://localhost:8080/","height":390},"outputId":"deebfd47-e670-476a-d756-cec416e4d97f"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e6ac3ffbc8bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               RandomSampler, SequentialSampler)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarmupLinearSchedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import torch\n","from torch.utils.data import (TensorDataset, DataLoader,\n","                              RandomSampler, SequentialSampler)\n","\n","from pytorch_transformers import BertTokenizer, BertConfig\n","from pytorch_transformers import BertForSequenceClassification\n","from pytorch_transformers import AdamW, WarmupLinearSchedule\n","\n","from distutils.version import LooseVersion as LV\n","\n","from sklearn.model_selection import train_test_split\n","\n","import io\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from datasets import load_dataset\n","\n","import tensorflow_datasets as tfds\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","\n","sns.set()\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    devicename = '['+torch.cuda.get_device_name(0)+']'\n","else:\n","    device = torch.device('cpu')\n","    devicename = \"\"\n","    \n","print('Using PyTorch version:', torch.__version__,\n","      'Device:', device, devicename)\n","assert(LV(torch.__version__) >= LV(\"1.0.0\"))\n"]},{"cell_type":"markdown","source":["Download the IMDb and SST-2 datasets and extract them."],"metadata":{"id":"PXhFhTFsYSGM"}},{"cell_type":"code","source":["# Load the IMDB dataset\n","imdb_dataset = load_dataset(\"imdb\")\n","\n","\n","# Load the SST-2 dataset\n","sst2_dataset = load_dataset(\"glue\", \"sst2\")"],"metadata":{"id":"S2w3DXq8zmL7","colab":{"base_uri":"https://localhost:8080/","height":137,"referenced_widgets":["78fe92eacae141e49ecf1ba64bf16d38","0ebb010186f149b8b2ddbeaaadab7f3b","165b4eb9f54f49cd8eda91e4edd2167f","ec66b493f9f0453aa20bf1b4e11f36b7","2c95e67ec2f4407d86c8ca5d6c65e13b","7271040f3a6341d3be21544e291383e3","c38a0ba549f64d93b078f7d92516e904","7b16614f618145de9defc26cedb89665","35173dfbabdc4a63bb3b588e678540f7","c0023d65922442b8acc4fce8315f5126","195d45f3d7ba4a8b9f86c305ddcdab3f","e8104f4bc65c4273b8a52d9d70dde3a2","40d246524ae54920bd978d8417cf6d2b","df081ba9d6de41bdb7c3928ccdaa3829","e3015666db7e48e78c9c2779cb6353cb","ab945ee5dc3f431dad5e280973f1e5ba","188170d6aa0b480eaf0cb224823e8bc7","84b860be3bbc46c891faf58f68235a76","e3fc82b32e514e9ba7c7a1f858e218e3","a077bb5e1f47411d8857be441bcdbabd","74bbab0a52e7411f8910593421c98ea6","f8e60611cf1f4c9381eefeb617a019cd"]},"executionInfo":{"status":"ok","timestamp":1678640144363,"user_tz":-60,"elapsed":2937,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"9501675a-81de-4856-8aa5-a48adccda126"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78fe92eacae141e49ecf1ba64bf16d38"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8104f4bc65c4273b8a52d9d70dde3a2"}},"metadata":{}}]},{"cell_type":"markdown","source":["Load the IMDb dataset using pandas, and preprocess the text data by removing HTML tags, non-alphanumeric characters, and stop words."],"metadata":{"id":"Iw5Lk4nFXf1A"}},{"cell_type":"code","source":["print(imdb_dataset.column_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VoEDXTDQr89F","executionInfo":{"status":"ok","timestamp":1678640145931,"user_tz":-60,"elapsed":324,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"578feefa-5023-48f7-cfa3-3cbbaf97f998"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'train': ['text', 'label'], 'test': ['text', 'label'], 'unsupervised': ['text', 'label']}\n"]}]},{"cell_type":"code","source":["# Load the IMDb dataset\n","imdb_df = pd.concat([pd.DataFrame(imdb_dataset['train']),pd.DataFrame(imdb_dataset['test'])])\n","imdb_df = imdb_df.reset_index(drop=True)\n","\n","\n","print('\\nIMDB data loaded:')\n","print('data set:', imdb_df.shape)\n","print(imdb_df['label'].unique())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXioOoLXj5_6","executionInfo":{"status":"ok","timestamp":1678640151162,"user_tz":-60,"elapsed":3280,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"549e1d92-e841-4395-ad42-39a484de99f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","IMDB data loaded:\n","data set: (50000, 2)\n","[0 1]\n"]}]},{"cell_type":"markdown","source":["Load the SST-2 dataset using pandas, and preprocess the text data in the same way as the IMDb dataset"],"metadata":{"id":"M5ajmdUSTPKj"}},{"cell_type":"code","source":["print(sst2_dataset.column_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPKsg4shr4zn","executionInfo":{"status":"ok","timestamp":1678640170549,"user_tz":-60,"elapsed":174,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"f3227a72-02d3-4729-c268-afed3c80c7bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'train': ['sentence', 'label', 'idx'], 'validation': ['sentence', 'label', 'idx'], 'test': ['sentence', 'label', 'idx']}\n"]}]},{"cell_type":"code","source":["# Load the SST-2 dataset\n","\n","sst2_df = pd.concat([pd.DataFrame(sst2_dataset['train'])[['sentence', 'label']],pd.DataFrame(sst2_dataset['validation'])[['sentence', 'label']]])\n","sst2_df = sst2_df.rename(columns={'sentence': 'text'})\n","sst2_df = sst2_df.reset_index(drop=True)\n","\n","\n","print('\\nSST2 data loaded:')\n","print('data set:', sst2_df.shape)\n","print(sst2_df['label'].unique())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sHU-_OllRtQ","executionInfo":{"status":"ok","timestamp":1678640177103,"user_tz":-60,"elapsed":4497,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"4d5c3f9d-faa3-4f00-8aa0-1e1e0da51354"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","SST2 data loaded:\n","data set: (68221, 2)\n","[0 1]\n"]}]},{"cell_type":"code","source":["# Preprocess the text data\n","sst2_df ['text'] = sst2_df ['text'].str.replace('<.*?>', '', regex=True) # remove HTML tags\n","sst2_df ['text'] = sst2_df ['text'].str.replace('[^a-zA-Z0-9\\s]', '', regex=True) # remove non-alphanumeric characters\n","stop_words = set(stopwords.words('english'))\n","sst2_df ['text'] = sst2_df ['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words])) # remove stop words"],"metadata":{"id":"EkwE9ffRA5YB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's view some random reviews:\n","print(sst2_df.sample(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJ2PQoK3BMxC","executionInfo":{"status":"ok","timestamp":1678640183784,"user_tz":-60,"elapsed":17,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"5e5dcea9-4e6f-41fa-a69d-ac60f9744f61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                    text  label\n","21013  sporadic bursts liveliness soso slapstick earp...      1\n","24055                                   got brawn brains      0\n","28152                       reflected almost every scene      1\n","38044                nothing less provocative piece work      1\n","9434                                       sharply comic      1\n"]}]},{"cell_type":"markdown","source":["Split into train and test set"],"metadata":{"id":"KSS15bBzr0lN"}},{"cell_type":"code","source":["# Define your features and target variable\n","X = sst2_df.drop(\"label\", axis=1)\n","y = sst2_df[\"label\"]\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Check the shape of the train and test sets\n","print(\"X_train shape:\", X_train.shape)\n","print(\"X_test shape:\", X_test.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"y_test shape:\", y_test.shape)\n","\n","sst2_train_df = pd.concat([X_train,y_train], axis=1)\n","sst2_test_df = pd.concat([X_test,y_test], axis=1)\n","\n","print('\\nSST2 data re splitted:')\n","print('train:', sst2_train_df.shape)\n","print('test:', sst2_test_df.shape)\n","print(sst2_train_df['label'].unique())\n","print(sst2_test_df['label'].unique())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trDNhh0nsDGL","executionInfo":{"status":"ok","timestamp":1678640185843,"user_tz":-60,"elapsed":206,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"4c3fbe46-7571-4a1e-93e4-0f9791144406"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape: (54576, 1)\n","X_test shape: (13645, 1)\n","y_train shape: (54576,)\n","y_test shape: (13645,)\n","\n","SST2 data re splitted:\n","train: (54576, 2)\n","test: (13645, 2)\n","[1 0]\n","[1 0]\n"]}]},{"cell_type":"code","source":["# Let's view some random reviews:\n","print(sst2_train_df.sample(5))\n","print(sst2_test_df.sample(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GnTy7rj631jm","executionInfo":{"status":"ok","timestamp":1678640188130,"user_tz":-60,"elapsed":292,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"e0ab23ab-9232-46f0-eb96-2eee231fe1ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                    text  label\n","9365                                                 fun      1\n","30038  wake saving private ryan black hawk soldiers l...      0\n","50673           lowkey labor love strikes resonant chord      1\n","55021                              artsy often pointless      0\n","19588               make delicate comingofage tale treat      1\n","                                               text  label\n","65391  slack execution italicizes absurdity premise      0\n","2873                          bighearted frequently      1\n","45196                       bogs surfeit characters      0\n","67630           chokes depiction uppercrust decorum      0\n","40568                               sparkling ideas      1\n"]}]},{"cell_type":"markdown","source":["IN-DS: SST2\n","OOD-DS: IMDB"],"metadata":{"id":"KnVplK7CuFem"}},{"cell_type":"code","source":["#Temporary limit the IN-DS size to 10%\n","n = 0.05\n","\n","train_df = sst2_train_df.sample(int(n*sst2_train_df.shape[0]))\n","test_df = sst2_test_df.sample(int(n*sst2_test_df.shape[0]))\n","\n","train_df = train_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","\n","n_ood = 0.3\n","ood_df = imdb_df.sample(int(n_ood*train_df.shape[0]))\n","ood_df = ood_df.reset_index(drop=True)"],"metadata":{"id":"stJtXnEyuE5h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocess the text data\n","ood_df ['text'] = ood_df ['text'].str.replace('<.*?>', '', regex=True) # remove HTML tags\n","ood_df ['text'] = ood_df ['text'].str.replace('[^a-zA-Z0-9\\s]', '', regex=True) # remove non-alphanumeric characters\n","stop_words = set(stopwords.words('english'))\n","ood_df ['text'] = ood_df ['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words])) # remove stop words\n","\n","\n","# Let's view some random reviews:\n","print(ood_df.sample(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbFj52jjon7X","executionInfo":{"status":"ok","timestamp":1678640192493,"user_tz":-60,"elapsed":22,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"44897aaf-7903-49bf-d152-3edd4c04ab7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  text  label\n","778  I say I curious viewing film considered notori...      0\n","142  A difficult film categorize I never giving 110...      0\n","740  Loosely based actual events Rivers Edge film m...      1\n","71   YesIm going 10 heres In last years I watched q...      1\n","165  I watched film times 90s nearly split sides la...      1\n"]}]},{"cell_type":"code","source":["del X_train, X_test, y_train, y_test"],"metadata":{"id":"fd4YYYJ2-OF1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The token `[CLS]` is a special token required by BERT at the beginning of the sentence."],"metadata":{"id":"1ZuRWQHKTbVn"}},{"cell_type":"code","source":["sentences_train = train_df.text.values\n","sentences_train = [\"[CLS] \" + s for s in sentences_train]\n","\n","sentences_test = test_df.text.values\n","sentences_test = [\"[CLS] \" + s for s in sentences_test]\n","\n","sentences_ood = ood_df.text.values\n","sentences_ood = [\"[CLS] \" + s for s in sentences_ood]\n","\n","\n","labels_train = train_df.label.values\n","labels_test  = test_df.label.values\n","labels_ood  = ood_df.label.values\n","\n","print (\"\\nThe first training sentence:\")\n","print(sentences_train[0], 'LABEL:', labels_train[0])\n"],"metadata":{"id":"UPKOd23EMA5h","executionInfo":{"status":"ok","timestamp":1678640198660,"user_tz":-60,"elapsed":191,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7d38fbf7-8cbd-4f78-9f41-3ae07818e5b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","The first training sentence:\n","[CLS] visual merits LABEL: 1\n"]}]},{"cell_type":"markdown","source":["Next we use the BERT tokenizer to convert the sentences into tokens\n","that match the data BERT was trained on.\n"],"metadata":{"id":"71yBR1tYTeFz"}},{"cell_type":"code","source":["BERTMODEL = \"bert-base-uncased\"\n","\n","tokenizer = BertTokenizer.from_pretrained(BERTMODEL,\n","                                          do_lower_case=True)\n","\n","tokenized_train = [tokenizer.tokenize(s) for s in sentences_train]\n","tokenized_test  = [tokenizer.tokenize(s) for s in sentences_test]\n","tokenized_ood  = [tokenizer.tokenize(s) for s in sentences_ood]\n","\n","print (\"\\nThe full tokenized first training sentence:\")\n","print (tokenized_train[0])\n","\n","print (\"\\nThe full tokenized first test sentence:\")\n","print (tokenized_test[0])\n","\n","print (\"\\nThe full tokenized first OOD sentence:\")\n","print (tokenized_ood[0])"],"metadata":{"id":"_pTzOWtWTe0w","executionInfo":{"status":"ok","timestamp":1678640203405,"user_tz":-60,"elapsed":2637,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0808f281-7088-4305-ffcc-a5135952cf1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","The full tokenized first training sentence:\n","['[CLS]', 'visual', 'merits']\n","\n","The full tokenized first test sentence:\n","['[CLS]', 'funny', 'yet', 'dark', 'seed', '##y', 'clash', 'cultures', 'generations']\n","\n","The full tokenized first OOD sentence:\n","['[CLS]', 'a', '2006', 'online', 'poll', 'japan', '##s', 'top', '100', 'favorite', 'animated', 'television', 'series', 'time', 'conducted', 'tv', 'asahi', 'placed', 'series', 'fourth', 'place', 'that', 'tells', 'everything', 'need', 'know', 'so', 'go', 'watch', 'i', 'won', '##t', 'comment', 'story', 'simply', 'i', 'don', '##t', 'want', 'ruin', 'anything', 'i', 'urge', 'keep', 'watching', 'saw', 'first', 'episode', 'the', 'animation', 'really', 'good', 'tv', 'average', 'the', 'best', 'thing', 'series', 'finally', 'something', 'new', 'i', 'mean', 'groundbreaking', 'still', 'offers', 'fresh', 'new', 'idea', 'li', '##ka', '##ble', 'characters', 'you', 'won', '##t', 'regret', 'entering', 'su', '##zumi', '##ya', 'ha', '##ru', '##his', 'world', '##and', 'best', 'thing', 'movie', 'coming', 'soon']\n"]}]},{"cell_type":"markdown","source":["\n","Now we set the maximum sequence lengths for our training and test\n","sentences as `MAX_LEN_TRAIN` and `MAX_LEN_TEST`. The maximum length\n","supported by the used BERT model is 512.\n","\n","The token `[SEP]` is another special token required by BERT at the\n","end of the sentence."],"metadata":{"id":"iGEGsXZLThMZ"}},{"cell_type":"code","source":["MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n","\n","tokenized_train = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized_train]\n","tokenized_test  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_test]\n","tokenized_ood  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_ood]\n","\n","print (\"\\nThe truncated tokenized first training sentence:\")\n","print (tokenized_train[0])"],"metadata":{"id":"YxtWll1wThtk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678640207072,"user_tz":-60,"elapsed":1293,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"634cd493-5131-448e-fa6a-634a8632d3a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","The truncated tokenized first training sentence:\n","['[CLS]', 'visual', 'merits', 'SEP']\n"]}]},{"cell_type":"markdown","source":["\n","Next we use the BERT tokenizer to convert each token into an integer\n","index in the BERT vocabulary. We also pad any shorter sequences to\n","`MAX_LEN_TRAIN` or `MAX_LEN_TEST` indices with trailing zeros."],"metadata":{"id":"2FDmuiZ5VaOQ"}},{"cell_type":"code","source":["ids_train = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_train]\n","ids_train = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)),\n","                             mode='constant') for i in ids_train])\n","\n","ids_test = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_test]\n","ids_test = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)),\n","                            mode='constant') for i in ids_test])\n","\n","\n","ids_ood = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_ood]\n","ids_ood = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)),\n","                            mode='constant') for i in ids_ood])\n","\n","print (\"\\nThe indices of the first training sentence:\")\n","print (ids_train[0])"],"metadata":{"id":"H0LgugShVasR","executionInfo":{"status":"ok","timestamp":1678640209742,"user_tz":-60,"elapsed":474,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"67480077-c8ed-4913-895f-683d62b80d8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","The indices of the first training sentence:\n","[  101  5107 22617   100     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0]\n"]}]},{"cell_type":"markdown","source":["BERT also requires *attention masks*, with 1 for each real token in\n","the sequences and 0 for the padding:"],"metadata":{"id":"KXHXkvs5JTet"}},{"cell_type":"code","source":["amasks_train, amasks_test , amasks_ood = [], [] , []\n","\n","for seq in ids_train:\n","  seq_mask = [float(i>0) for i in seq]\n","  amasks_train.append(seq_mask)\n","\n","for seq in ids_test:\n","  seq_mask = [float(i>0) for i in seq]\n","  amasks_test.append(seq_mask)\n","\n","\n","for seq in ids_ood:\n","  seq_mask = [float(i>0) for i in seq]\n","  amasks_ood.append(seq_mask)"],"metadata":{"id":"2yaiNP45Dkcm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We use scikit-learn's train_test_split() to use 10% of our training\n","data as a validation set, and then convert all data into\n","torch.tensors."],"metadata":{"id":"Ko7iCp5cJiyd"}},{"cell_type":"code","source":["(train_inputs, validation_inputs,\n"," train_labels, validation_labels) = train_test_split(ids_train, labels_train,\n","                                                     random_state=42,\n","                                                     test_size=0.1)\n","(train_masks, validation_masks,\n"," _, _) = train_test_split(amasks_train, ids_train,\n","                          random_state=42, test_size=0.1)\n","\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks  = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks  = torch.tensor(validation_masks)\n","test_inputs = torch.tensor(ids_test)\n","test_labels = torch.tensor(labels_test)\n","test_masks  = torch.tensor(amasks_test)\n","ood_inputs = torch.tensor(ids_ood)\n","ood_labels = torch.tensor(labels_ood)\n","ood_masks  = torch.tensor(amasks_ood)\n","\n"],"metadata":{"id":"p3djaDPDJnZ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we create PyTorch *DataLoader*s for all data sets.\n","For fine-tuning BERT on a specific task, the authors recommend a\n","batch size of 16 or 32."],"metadata":{"id":"a4TDX7VtJ2aA"}},{"cell_type":"code","source":["BATCH_SIZE = 8\n","\n","print('\\nDatasets:')\n","print('Train: ', end=\"\")\n","train_data = TensorDataset(train_inputs, train_masks,\n","                           train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler,\n","                              batch_size=BATCH_SIZE)\n","print(len(train_data), 'reviews')\n","\n","print('Validation: ', end=\"\")\n","validation_data = TensorDataset(validation_inputs, validation_masks,\n","                                validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data,\n","                                   sampler=validation_sampler,\n","                                   batch_size=BATCH_SIZE)\n","print(len(validation_data), 'reviews')\n","\n","print('Test: ', end=\"\")\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler,\n","                             batch_size=BATCH_SIZE)\n","print(len(test_data), 'reviews')\n","\n","\n","print('OOD: ', end=\"\")\n","ood_data = TensorDataset(ood_inputs, ood_masks, ood_labels)\n","ood_sampler = SequentialSampler(ood_data)\n","ood_dataloader = DataLoader(ood_data, sampler=ood_sampler,\n","                             batch_size=BATCH_SIZE)\n","print(len(ood_data), 'reviews')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AjgT4K4J2Fv","executionInfo":{"status":"ok","timestamp":1678640220564,"user_tz":-60,"elapsed":179,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"e4efd3d2-ea24-4920-fb97-16cc47215b83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Datasets:\n","Train: 2455 reviews\n","Validation: 273 reviews\n","Test: 682 reviews\n","OOD: 818 reviews\n"]}]},{"cell_type":"markdown","source":["BERT MODEL INITIALIZATION\n","\n","We now load a pretrained BERT model with a single linear\n","classification layer added on top.\n"],"metadata":{"id":"a4Wi5wQqKEuc"}},{"cell_type":"code","source":["model = BertForSequenceClassification.from_pretrained(BERTMODEL,\n","                                                      num_labels=2,\n","                                                      output_hidden_states=True)\n","\n","\n","model.cuda()\n","print('\\nPretrained BERT model \"{}\" loaded'.format(BERTMODEL))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdIeyBLOJ2DE","executionInfo":{"status":"ok","timestamp":1678640228789,"user_tz":-60,"elapsed":5819,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"401a1f7c-723f-4322-ede3-fcf3e1322238"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Pretrained BERT model \"bert-base-uncased\" loaded\n"]}]},{"cell_type":"markdown","source":["\n","We set the remaining hyperparameters needed for fine-tuning the\n","pretrained model: \n"," * EPOCHS: the number of training epochs in fine-tuning\n","   (recommended values between 2 and 4) \n"," * WEIGHT_DECAY: weight decay for the Adam optimizer \n"," * LR: learning rate for the Adam optimizer \n","   (2e-5 to 5e-5 recommended) \n"," * WARMUP_STEPS: number of warmup steps to (linearly) reach the\n","   set learning rate\n","\n"," We also need to grab the training parameters from the pretrained\n"," model."],"metadata":{"id":"ynTgfHzeLUXY"}},{"cell_type":"code","source":["EPOCHS = 4\n","WEIGHT_DECAY = 0.01\n","LR = 2e-5\n","WARMUP_STEPS =int(0.2*len(train_dataloader))\n","\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters()\n","                if not any(nd in n for nd in no_decay)],\n","     'weight_decay': WEIGHT_DECAY},\n","    {'params': [p for n, p in model.named_parameters()\n","                if any(nd in n for nd in no_decay)],\n","     'weight_decay': 0.0}\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n","scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS,\n","                                 t_total=len(train_dataloader)*EPOCHS)"],"metadata":{"id":"ZSPy9RhsJ2AT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LEARNING\n","\n","Let's now define functions to train() and evaluate() the model:"],"metadata":{"id":"zpTM0QR9Lyee"}},{"cell_type":"code","source":["def train(epoch, loss_vector=None, log_interval=200):\n","    # Set model to training mode\n","    model.train().to(device)\n","\n","    # Loop over each batch from the training set\n","    for step, batch in enumerate(train_dataloader):\n","        # Copy data to GPU if needed\n","        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Zero gradient buffers\n","        optimizer.zero_grad()\n","\n","        with torch.set_grad_enabled(True):\n","            # Forward pass\n","            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,\n","                         labels=b_labels)[0]\n","\n","        if loss_vector is not None:\n","            loss_vector.append(loss.item())\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update weights\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Clear unused variables\n","        del  b_input_mask, b_labels\n","\n","        if step % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                  epoch, step * len(b_input_ids), len(train_dataloader.dataset),\n","                  100. * step / len(train_dataloader), loss.item()))\n","            \n","    # Clear unused variables\n","    del b_input_ids,batch, loss\n"],"metadata":{"id":"XBbxjNth2BoE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(loader):\n","  model.eval()\n","\n","  n_correct, n_all = 0, 0\n","\n","  for batch in loader:\n","    batch = tuple(t.to(device) for t in batch)\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None,\n","                      attention_mask=b_input_mask)\n","      logits = outputs[0]\n","\n","    logits = logits.detach().cpu().numpy()\n","    predictions = np.argmax(logits, axis=1)\n","\n","    labels = b_labels.to('cpu').numpy()\n","    n_correct += np.sum(predictions == labels)\n","    n_all += len(labels)\n","\n","  print('Accuracy: [{}/{}] {:.4f}'.format(n_correct, n_all,\n","                                          n_correct/n_all))\n","\n","\n","\n","    "],"metadata":{"id":"6S9746L21fMw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we are ready to train our model using the train()\n","function. After each epoch, we evaluate the model using the\n","validation set and evaluate()."],"metadata":{"id":"FMuuXi6lL65Q"}},{"cell_type":"code","source":["train_lossv = []\n","for epoch in range(1, EPOCHS + 1):\n","    print()\n","    train(epoch, train_lossv)\n","    print('\\nValidation set:')\n","    evaluate(validation_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-TeGUafJ16j","executionInfo":{"status":"ok","timestamp":1678640519242,"user_tz":-60,"elapsed":261540,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"7880e0b4-24a0-4ea1-ba6e-ce45a5d0e21f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/2455 (0%)]\tLoss: 0.852963\n","Train Epoch: 1 [1600/2455 (65%)]\tLoss: 0.543897\n","\n","Validation set:\n","Accuracy: [209/273] 0.7656\n","\n","Train Epoch: 2 [0/2455 (0%)]\tLoss: 0.141186\n","Train Epoch: 2 [1600/2455 (65%)]\tLoss: 0.387481\n","\n","Validation set:\n","Accuracy: [214/273] 0.7839\n","\n","Train Epoch: 3 [0/2455 (0%)]\tLoss: 0.049431\n","Train Epoch: 3 [1600/2455 (65%)]\tLoss: 0.108106\n","\n","Validation set:\n","Accuracy: [217/273] 0.7949\n","\n","Train Epoch: 4 [0/2455 (0%)]\tLoss: 0.095080\n","Train Epoch: 4 [1600/2455 (65%)]\tLoss: 0.020699\n","\n","Validation set:\n","Accuracy: [220/273] 0.8059\n"]}]},{"cell_type":"markdown","source":["Let's take a look at our training loss over all batches:"],"metadata":{"id":"RbJJCz8GMDt1"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(15,8))\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(train_lossv, label='original')\n","plt.plot(np.convolve(train_lossv, np.ones(101), 'same') / 101,\n","         label='averaged')\n","plt.legend(loc='best')\n","plt.savefig(\"training-loss.png\")\n","plt.show()\n"],"metadata":{"id":"YehHezB-J13b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inference\n","\n","For a better measure of the quality of the model, let's see the\n","model accuracy for the test reviews."],"metadata":{"id":"cZ60h-XMMQcK"}},{"cell_type":"code","source":["print('\\nTest set:')\n","evaluate(test_dataloader)\n","# eof"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OoW9yYglJ1vs","executionInfo":{"status":"ok","timestamp":1678640550887,"user_tz":-60,"elapsed":23367,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"outputId":"b6083359-02b9-4ca3-f058-6d6624a9cd79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test set:\n","Accuracy: [577/682] 0.8460\n"]}]},{"cell_type":"markdown","source":["# OOD detection "],"metadata":{"id":"J5EOhawoaTit"}},{"cell_type":"markdown","source":["Extract BERT features"],"metadata":{"id":"u_2e2L6da_xj"}},{"cell_type":"code","source":["def extract_bert_features(loader):\n","    model.eval()\n","\n","    last_layer_list = []\n","    logits_list = []\n","    softmax_list = []\n","    label_list = []\n","    pred_list = []\n","\n","    for batch in loader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask)\n","            last_layer = outputs[-1]\n","            logits = outputs[0]\n","            predictions = np.argmax(logits.detach().cpu().numpy(), axis=1)\n","            softmax = torch.nn.functional.softmax(logits, dim=-1)\n","\n","        last_layer_list.append(last_layer)\n","        logits_list.append(logits.cpu().numpy())\n","        softmax_list.append(softmax.cpu().numpy())\n","        label_list.append(b_labels.cpu().numpy())\n","        pred_list.append(predictions)\n","    \n","\n","        \n","\n","    last_layer = torch.cat(last_layer_list, dim=0)\n","    logits = np.concatenate(logits_list, axis=0)\n","    softmax = np.concatenate(softmax_list, axis=0)\n","    labels = np.concatenate(label_list, axis=0)\n","    predictions = np.concatenate(pred_list, axis=0)\n","\n","    return last_layer.to('cpu').numpy(), logits, softmax, labels, predictions\n","    \n","\n"],"metadata":{"id":"1VuC8taNaTHb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimized memory version to be tested"],"metadata":{"id":"UomewBAh1HtH"}},{"cell_type":"code","source":["\"\"\"\n","\n","def extract_bert_features(loader):\n","    model.eval()\n","\n","    last_layer_list = []\n","    logits_list = []\n","    softmax_list = []\n","    label_list = []\n","    pred_list = []\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","\n","            outputs = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask)\n","            last_layer = outputs[-1]\n","            logits = outputs[0].detach().cpu().numpy()\n","            predictions = np.argmax(logits, axis=1)\n","            softmax = torch.nn.functional.softmax(outputs[0], dim=-1).cpu().numpy()\n","\n","            last_layer_list.append(last_layer.cpu())\n","            logits_list.append(logits)\n","            softmax_list.append(softmax)\n","            label_list.append(b_labels.detach().cpu().numpy())\n","            pred_list.append(predictions)\n","\n","    last_layer = torch.cat(last_layer_list, dim=0).numpy()\n","    labels = np.concatenate(label_list, axis=0)\n","    predictions = np.concatenate(pred_list, axis=0)\n","\n","    return last_layer.to('cpu').numpy(), logits_list, softmax_list, labels, predictions\n","\n","    \"\"\"\n"],"metadata":{"id":"PDIo_Jxxv03O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_features = extract_bert_features(train_dataloader)\n","test_features = extract_bert_features(test_dataloader)\n","ood_features = extract_bert_features(ood_dataloader)"],"metadata":{"id":"p0tBrqYkbDJk","executionInfo":{"status":"error","timestamp":1678640607721,"user_tz":-60,"elapsed":20905,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}},"colab":{"base_uri":"https://localhost:8080/","height":414},"outputId":"5be00a89-a0f4-4f2d-a425-8c2a44053047"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-dff85d8244c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train_features = extract_bert_features(train_dataloader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_bert_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#ood_features = extract_bert_features(ood_dataloader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-c0816c0b440e>\u001b[0m in \u001b[0;36mextract_bert_features\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             outputs = model(b_input_ids, token_type_ids=None,\n\u001b[0m\u001b[1;32m     16\u001b[0m                             attention_mask=b_input_mask)\n\u001b[1;32m     17\u001b[0m             \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, position_ids, head_mask)\u001b[0m\n\u001b[1;32m    960\u001b[0m     def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n\u001b[1;32m    961\u001b[0m                 position_ids=None, head_mask=None):\n\u001b[0;32m--> 962\u001b[0;31m         outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n\u001b[0m\u001b[1;32m    963\u001b[0m                             attention_mask=attention_mask, head_mask=head_mask)\n\u001b[1;32m    964\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, head_mask)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         encoder_outputs = self.encoder(embedding_output,\n\u001b[0m\u001b[1;32m    709\u001b[0m                                        \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                        head_mask=head_mask)\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mattention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 14.75 GiB total capacity; 13.16 GiB already allocated; 80.81 MiB free; 13.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"markdown","source":["## OOD using Mahalanobis distance on last layer"],"metadata":{"id":"DnC_LAB21srU"}},{"cell_type":"markdown","source":["Covariance matrix for the latent features of the internal distribution (SST2)"],"metadata":{"id":"7V3U1oUC1wdJ"}},{"cell_type":"code","source":["train_mean = np.mean(train_features, axis=0)\n","train_cov = np.cov(train_features, rowvar=False)\n","train_inv_cov = np.linalg.inv(train_cov)\n"],"metadata":{"id":"j4KWp1Gfi3DF","executionInfo":{"status":"ok","timestamp":1678641620294,"user_tz":-60,"elapsed":14,"user":{"displayName":"Andres Fernando Garcia Parrado","userId":"07689739672482382229"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculation of the Mahalanobis distance on the in-ds and ood data"],"metadata":{"id":"qZsZetrt11cD"}},{"cell_type":"code","source":["from scipy.spatial.distance import mahalanobis\n","\n","inds_test_scores = []\n","ood_test_scores = []\n","\n","for feature in test_features:\n","    score = mahalanobis(feature, train_mean, train_inv_cov)\n","    inds_test_scores.append(score)\n","\n","for feature in ood_features:\n","    score = mahalanobis(feature, train_mean, train_inv_cov)\n","    ood_test_scores.append(score)"],"metadata":{"id":"BhuapRR_jvwX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will evaluate the performance of OOD detection using the AUROC, AUPR, FPR and ERR metrics"],"metadata":{"id":"jUzHDgqk2CCM"}},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n","\n","labels = np.concatenate([np.zeros(len(inds_test_scores)), np.ones(len(ood_test_scores))])\n","scores = np.concatenate([inds_test_scores, ood_test_scores])\n","\n","#define the threshold for ood detection\n","threshold = np.mean(inds_test_scores) +  np.std(inds_test_scores)"],"metadata":{"id":"gCPMaZbb2B1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def metrics(scores: np.ndarray, labels: np.ndarray, threshold: float):\n","    \"\"\"Computes the number of\n","        * true positives,\n","        * false positives,\n","        * true negatives,\n","        * false negatives,\n","        * the accuracy\n","        * false positive rate\n","        * taux d'erreur\n","    for a given threshold\n","\n","    \"\"\"\n","    pos = np.where(scores >= threshold)\n","    neg = np.where(scores < threshold)\n","    n_pos = len(pos[0])\n","    n_neg = len(neg[0])\n","\n","    tp = np.sum(labels[pos])\n","    fp = n_pos - tp\n","    fn = np.sum(labels[neg])\n","    tn = n_neg - fn\n","\n","    FPR = fp / (fp + tn)\n","    Accuracy = (tp+tn)/len(scores)\n","    ERR = 1 - Accuracy\n","\n","    return FPR, ERR\n","\n","FPR, ERR = metrics (scores, labels, threshold)\n","auroc = roc_auc_score(labels, scores)\n","aupr = average_precision_score(labels, scores)\n","print('AUROC:', auroc)\n","print('AUPR:', aupr)\n","print('FPR:', FPR)\n","print('ERR:', ERR)"],"metadata":{"id":"yrScAe6-2IkM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fpr, tpr, _ = roc_curve(labels, scores)\n","precision, recall, _ = precision_recall_curve(labels, scores)\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(121)\n","plt.plot(fpr, tpr)\n","plt.plot([0, 1], [0, 1], linestyle='--')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('AUROC Curve')\n","\n","plt.subplot(122)\n","plt.plot(recall, precision)\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('AUPR Curve')\n","\n","plt.tight_layout()\n","plt.savefig(\"auroc_aupr_mahalabinos.png\")\n","plt.show()"],"metadata":{"id":"-QAZex7S2Lio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## OOD detection using Energy-based score\n"],"metadata":{"id":"5VNjfXwq2OJN"}},{"cell_type":"code","source":[],"metadata":{"id":"tkRz5Ima2Oeo"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1W6mUzJV-FZrp4gV_Cquq8jm9Qw3ZhyTU","authorship_tag":"ABX9TyO+O+hgzWtjSRz/yaGK3ryc"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"78fe92eacae141e49ecf1ba64bf16d38":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ebb010186f149b8b2ddbeaaadab7f3b","IPY_MODEL_165b4eb9f54f49cd8eda91e4edd2167f","IPY_MODEL_ec66b493f9f0453aa20bf1b4e11f36b7"],"layout":"IPY_MODEL_2c95e67ec2f4407d86c8ca5d6c65e13b"}},"0ebb010186f149b8b2ddbeaaadab7f3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7271040f3a6341d3be21544e291383e3","placeholder":"​","style":"IPY_MODEL_c38a0ba549f64d93b078f7d92516e904","value":"100%"}},"165b4eb9f54f49cd8eda91e4edd2167f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b16614f618145de9defc26cedb89665","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35173dfbabdc4a63bb3b588e678540f7","value":3}},"ec66b493f9f0453aa20bf1b4e11f36b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0023d65922442b8acc4fce8315f5126","placeholder":"​","style":"IPY_MODEL_195d45f3d7ba4a8b9f86c305ddcdab3f","value":" 3/3 [00:00&lt;00:00, 56.60it/s]"}},"2c95e67ec2f4407d86c8ca5d6c65e13b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7271040f3a6341d3be21544e291383e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c38a0ba549f64d93b078f7d92516e904":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b16614f618145de9defc26cedb89665":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35173dfbabdc4a63bb3b588e678540f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c0023d65922442b8acc4fce8315f5126":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"195d45f3d7ba4a8b9f86c305ddcdab3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8104f4bc65c4273b8a52d9d70dde3a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_40d246524ae54920bd978d8417cf6d2b","IPY_MODEL_df081ba9d6de41bdb7c3928ccdaa3829","IPY_MODEL_e3015666db7e48e78c9c2779cb6353cb"],"layout":"IPY_MODEL_ab945ee5dc3f431dad5e280973f1e5ba"}},"40d246524ae54920bd978d8417cf6d2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_188170d6aa0b480eaf0cb224823e8bc7","placeholder":"​","style":"IPY_MODEL_84b860be3bbc46c891faf58f68235a76","value":"100%"}},"df081ba9d6de41bdb7c3928ccdaa3829":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3fc82b32e514e9ba7c7a1f858e218e3","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a077bb5e1f47411d8857be441bcdbabd","value":3}},"e3015666db7e48e78c9c2779cb6353cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74bbab0a52e7411f8910593421c98ea6","placeholder":"​","style":"IPY_MODEL_f8e60611cf1f4c9381eefeb617a019cd","value":" 3/3 [00:00&lt;00:00, 50.83it/s]"}},"ab945ee5dc3f431dad5e280973f1e5ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"188170d6aa0b480eaf0cb224823e8bc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84b860be3bbc46c891faf58f68235a76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3fc82b32e514e9ba7c7a1f858e218e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a077bb5e1f47411d8857be441bcdbabd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74bbab0a52e7411f8910593421c98ea6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8e60611cf1f4c9381eefeb617a019cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}