{"cells":[{"cell_type":"markdown","metadata":{"id":"8QqJbxx-_BU7"},"source":["#Install libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Fy54k2O-Q3N"},"outputs":[],"source":["!pip install transformers\n","!pip install torch torchvision\n","!pip install pandas\n","!pip install numpy\n","!pip install datasets\n","!pip install pytorch_transformers\n","!pip install scikit-learn\n","!pip install matplotlib\n","!pip install seaborn\n","!pip install nltk"]},{"cell_type":"markdown","metadata":{"id":"J13iSqp__qMw"},"source":["Import the required libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"o-shbksg4Wz2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"596d9bef-d831-460f-eaf6-900cd74efbdf","executionInfo":{"status":"ok","timestamp":1678689464658,"user_tz":-60,"elapsed":8653,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Using PyTorch version: 1.13.1+cu116 Device: cuda [NVIDIA A100-SXM4-40GB]\n"]}],"source":["import torch\n","from torch.utils.data import (TensorDataset, DataLoader,\n","                              RandomSampler, SequentialSampler)\n","\n","from pytorch_transformers import BertTokenizer, BertConfig\n","from pytorch_transformers import BertForSequenceClassification\n","from pytorch_transformers import AdamW, WarmupLinearSchedule\n","\n","from distutils.version import LooseVersion as LV\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n","import torch.nn.functional as F\n","import io\n","from scipy.spatial.distance import mahalanobis\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from datasets import load_dataset\n","\n","import tensorflow_datasets as tfds\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","\n","sns.set()\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    devicename = '['+torch.cuda.get_device_name(0)+']'\n","else:\n","    device = torch.device('cpu')\n","    devicename = \"\"\n","    \n","print('Using PyTorch version:', torch.__version__,\n","      'Device:', device, devicename)\n","assert(LV(torch.__version__) >= LV(\"1.0.0\"))\n"]},{"cell_type":"markdown","source":["Download the IMDb and SST-2 datasets and extract them."],"metadata":{"id":"PXhFhTFsYSGM"}},{"cell_type":"code","source":["# Load the IMDB dataset\n","imdb_dataset = load_dataset(\"imdb\")\n","\n","\n","# Load the SST-2 dataset\n","sst2_dataset = load_dataset(\"glue\", \"sst2\")"],"metadata":{"id":"S2w3DXq8zmL7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the IMDb dataset using pandas, and preprocess the text data by removing HTML tags, non-alphanumeric characters, and stop words."],"metadata":{"id":"Iw5Lk4nFXf1A"}},{"cell_type":"code","source":["print(imdb_dataset.column_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VoEDXTDQr89F","outputId":"517d9395-9edb-41ab-da8d-c385eb0c00f3","executionInfo":{"status":"ok","timestamp":1678689534484,"user_tz":-60,"elapsed":38,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'train': ['text', 'label'], 'test': ['text', 'label'], 'unsupervised': ['text', 'label']}\n"]}]},{"cell_type":"code","source":["# Load the IMDb dataset\n","imdb_df = pd.concat([pd.DataFrame(imdb_dataset['train']),pd.DataFrame(imdb_dataset['test'])])\n","imdb_df = imdb_df.reset_index(drop=True)\n","\n","\n","print('\\nIMDB data loaded:')\n","print('data set:', imdb_df.shape)\n","print(imdb_df['label'].unique())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"dXioOoLXj5_6","outputId":"83a6c184-5465-4999-9508-ee35f018d7dd","executionInfo":{"status":"ok","timestamp":1678689535784,"user_tz":-60,"elapsed":1335,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","IMDB data loaded:\n","data set: (50000, 2)\n","[0 1]\n"]}]},{"cell_type":"markdown","source":["Load the SST-2 dataset using pandas, and preprocess the text data in the same way as the IMDb dataset"],"metadata":{"id":"M5ajmdUSTPKj"}},{"cell_type":"code","source":["print(sst2_dataset.column_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPKsg4shr4zn","outputId":"b4e77ccc-4ed4-4822-f1d7-5c8129190aef","executionInfo":{"status":"ok","timestamp":1678689535785,"user_tz":-60,"elapsed":21,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{'train': ['sentence', 'label', 'idx'], 'validation': ['sentence', 'label', 'idx'], 'test': ['sentence', 'label', 'idx']}\n"]}]},{"cell_type":"code","source":["# Load the SST-2 dataset\n","\n","sst2_df = pd.concat([pd.DataFrame(sst2_dataset['train'])[['sentence', 'label']],pd.DataFrame(sst2_dataset['validation'])[['sentence', 'label']]])\n","sst2_df = sst2_df.rename(columns={'sentence': 'text'})\n","sst2_df = sst2_df.reset_index(drop=True)\n","\n","\n","print('\\nSST2 data loaded:')\n","print('data set:', sst2_df.shape)\n","print(sst2_df['label'].unique())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sHU-_OllRtQ","outputId":"fc0bf0eb-bee4-4fd0-8f64-de728633c78c","executionInfo":{"status":"ok","timestamp":1678689538354,"user_tz":-60,"elapsed":2585,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","SST2 data loaded:\n","data set: (68221, 2)\n","[0 1]\n"]}]},{"cell_type":"code","source":["# Preprocess the text data\n","sst2_df ['text'] = sst2_df ['text'].str.replace('<.*?>', '', regex=True) # remove HTML tags\n","sst2_df ['text'] = sst2_df ['text'].str.replace('[^a-zA-Z0-9\\s]', '', regex=True) # remove non-alphanumeric characters\n","stop_words = set(stopwords.words('english'))\n","sst2_df ['text'] = sst2_df ['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words])) # remove stop words"],"metadata":{"id":"EkwE9ffRA5YB","executionInfo":{"status":"ok","timestamp":1678689538355,"user_tz":-60,"elapsed":30,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Let's view some random reviews:\n","print(sst2_df.sample(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJ2PQoK3BMxC","outputId":"9dba123a-3d25-410f-8655-47753bbfc0bb","executionInfo":{"status":"ok","timestamp":1678689538356,"user_tz":-60,"elapsed":29,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                    text  label\n","20138                            finally aged past prime      0\n","10553                       psychologically unpersuasive      0\n","43507                                 particularly funny      1\n","43905  weird thinking bad things world like puppies b...      0\n","33068   silly gross rarely moronic campus grossout films      0\n"]}]},{"cell_type":"markdown","source":["Split into train and test set"],"metadata":{"id":"KSS15bBzr0lN"}},{"cell_type":"code","source":["# Define your features and target variable\n","X = sst2_df.drop(\"label\", axis=1)\n","y = sst2_df[\"label\"]\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Check the shape of the train and test sets\n","print(\"X_train shape:\", X_train.shape)\n","print(\"X_test shape:\", X_test.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"y_test shape:\", y_test.shape)\n","\n","train_df = pd.concat([X_train,y_train], axis=1)\n","test_df = pd.concat([X_test,y_test], axis=1)\n","\n","print('\\nSST2 data re splitted:')\n","print('train:', train_df.shape)\n","print('test:', test_df.shape)\n","print(train_df['label'].unique())\n","print(test_df['label'].unique())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trDNhh0nsDGL","outputId":"854e0a0d-6da1-437a-9bf1-9753608c9710","executionInfo":{"status":"ok","timestamp":1678689538357,"user_tz":-60,"elapsed":28,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape: (54576, 1)\n","X_test shape: (13645, 1)\n","y_train shape: (54576,)\n","y_test shape: (13645,)\n","\n","SST2 data re splitted:\n","train: (54576, 2)\n","test: (13645, 2)\n","[1 0]\n","[1 0]\n"]}]},{"cell_type":"code","source":["# Let's view some random reviews:\n","print(train_df.sample(5))\n","print(test_df.sample(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GnTy7rj631jm","outputId":"e7c9a451-b448-4e01-f758-8baf5a2329c2","executionInfo":{"status":"ok","timestamp":1678689538358,"user_tz":-60,"elapsed":25,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                    text  label\n","27201               writing direction soggy performances      0\n","54424                     night living room night movies      0\n","10115  cho latest comic set nt sharp fresh one want s...      1\n","62586                             always hilarious meara      1\n","17512                                              savor      1\n","                                                    text  label\n","67832  even nt think kissinger guilty criminal activi...      1\n","17089                                              hokey      0\n","21626                                 gifted pearce hand      1\n","11056  scott convincing portrayal roger sad cad reall...      1\n","24991                                        overbearing      0\n"]}]},{"cell_type":"markdown","source":["IN-DS: SST2\n","OOD-DS: IMDB"],"metadata":{"id":"KnVplK7CuFem"}},{"cell_type":"code","source":["\n","n_ood = 0.1\n","ood_df = imdb_df.sample(int(n_ood*train_df.shape[0]))\n","ood_df = ood_df.reset_index(drop=True)"],"metadata":{"id":"stJtXnEyuE5h","executionInfo":{"status":"ok","timestamp":1678689538359,"user_tz":-60,"elapsed":24,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Preprocess the text data\n","ood_df ['text'] = ood_df ['text'].str.replace('<.*?>', '', regex=True) # remove HTML tags\n","ood_df ['text'] = ood_df ['text'].str.replace('[^a-zA-Z0-9\\s]', '', regex=True) # remove non-alphanumeric characters\n","stop_words = set(stopwords.words('english'))\n","ood_df ['text'] = ood_df ['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words])) # remove stop words\n","\n","\n","# Let's view some random reviews:\n","print(ood_df.sample(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbFj52jjon7X","outputId":"4d4e3604-e652-431e-cfe2-4a9d8085d684","executionInfo":{"status":"ok","timestamp":1678689539001,"user_tz":-60,"elapsed":665,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                   text  label\n","882   haggard doesnt even need graded since never de...      1\n","201   This one best movies come HK long time eagerly...      1\n","2107  This movie kind reminds A MaryKate Ashley movi...      0\n","5111  I actually planning see movie I noticed TV gui...      0\n","133   I peeved best makeup academy award went Dick T...      1\n"]}]},{"cell_type":"code","source":["del X_train, X_test, y_train, y_test"],"metadata":{"id":"fd4YYYJ2-OF1","executionInfo":{"status":"ok","timestamp":1678689539002,"user_tz":-60,"elapsed":22,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["The token `[CLS]` is a special token required by BERT at the beginning of the sentence."],"metadata":{"id":"1ZuRWQHKTbVn"}},{"cell_type":"code","source":["sentences_train = train_df.text.values\n","sentences_train = [\"[CLS] \" + s for s in sentences_train]\n","\n","sentences_test = test_df.text.values\n","sentences_test = [\"[CLS] \" + s for s in sentences_test]\n","\n","sentences_ood = ood_df.text.values\n","sentences_ood = [\"[CLS] \" + s for s in sentences_ood]\n","\n","\n","labels_train = train_df.label.values\n","labels_test  = test_df.label.values\n","labels_ood  = ood_df.label.values\n","\n","print (\"\\nThe first training sentence:\")\n","print(sentences_train[0], 'LABEL:', labels_train[0])\n"],"metadata":{"id":"UPKOd23EMA5h","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6f728cd5-c9e7-4ef1-b266-35c195614514","executionInfo":{"status":"ok","timestamp":1678689539002,"user_tz":-60,"elapsed":20,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","The first training sentence:\n","[CLS] wildly alive LABEL: 1\n"]}]},{"cell_type":"markdown","source":["Next we use the BERT tokenizer to convert the sentences into tokens\n","that match the data BERT was trained on.\n"],"metadata":{"id":"71yBR1tYTeFz"}},{"cell_type":"code","source":["BERTMODEL = \"bert-base-uncased\"\n","\n","tokenizer = BertTokenizer.from_pretrained(BERTMODEL,\n","                                          do_lower_case=True)\n","\n","tokenized_train = [tokenizer.tokenize(s) for s in sentences_train]\n","tokenized_test  = [tokenizer.tokenize(s) for s in sentences_test]\n","tokenized_ood  = [tokenizer.tokenize(s) for s in sentences_ood]\n","\n","print (\"\\nThe full tokenized first training sentence:\")\n","print (tokenized_train[0])\n","\n","print (\"\\nThe full tokenized first test sentence:\")\n","print (tokenized_test[0])\n","\n","print (\"\\nThe full tokenized first OOD sentence:\")\n","print (tokenized_ood[0])"],"metadata":{"id":"_pTzOWtWTe0w","colab":{"base_uri":"https://localhost:8080/","height":645},"outputId":"e3135004-6afe-4782-814e-ae15a6dc07ee","executionInfo":{"status":"ok","timestamp":1678689564110,"user_tz":-60,"elapsed":25124,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","\n","\n","\n","\n","  0%|          | 0/231508 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  4%|▎         | 8192/231508 [00:00<00:05, 37234.62B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n"," 19%|█▊        | 43008/231508 [00:00<00:01, 107220.59B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n"," 41%|████      | 95232/231508 [00:00<00:00, 164899.61B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","100%|██████████| 231508/231508 [00:00<00:00, 257774.58B/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","The full tokenized first training sentence:\n","['[CLS]', 'wildly', 'alive']\n","\n","The full tokenized first test sentence:\n","['[CLS]', 'best', 'script']\n","\n","The full tokenized first OOD sentence:\n","['[CLS]', 'two', 'horse', 'traders', 'arrive', 'town', 'meet', 'leader', 'group', 'mormon', '##s', 'bound', 'valley', 'settle', 'live', 'peace', 'the', 'scenes', 'co', '##rral', 'town', 'ward', 'bond', 'ben', 'johnson', 'negotiate', 'prices', 'bond', 'introduces', 'idea', 'johnson', 'partner', 'played', 'harry', 'carey', 'jr', 'leading', 'train', 'valley', 'best', 'film', 'johnson', 'real', 'cowboy', 'w', '##hit', '##tle', '##s', 'piece', 'wood', 'ban', '##ters', 'bond', 'once', 'trail', 'come', 'upon', 'joanne', 'dr', '##u', 'maybe', 'john', 'ford', 'saw', 'red', 'river', 'offered', 'much', 'better', 'part', 'film', 'in', 'mor', '##man', 'train', 'number', 'notable', 'characters', 'the', 'mor', '##mans', 'peace', '##able', 'group', 'challenged', 'along', 'way', 'truly', 'low', '##life', 'group', 'outlaws', 'in', 'case', 'outlaws', 'case', 'people', 'train', 'later', 'band', 'navajo', '##s', 'encounter', 'well', 'written', 'characters', 'played', 'ben', 'johnson', 'ward', 'bond', 'film', 'completely', 'evade', '##s', 'stereotypes', 'camera', 'seems', 'spend', 'much', 'time', 'giving', 'viewer', 'big', 'picture', 'monument', 'valley', 'framing', 'train', 'moves', 'along', 'water', 'crossings', 'along', 'way', 'stunning', 'black', 'white', 'coming', 'back', 'what', '##s', 'happening', 'rolling', 'community', 'accompaniment', 'beautiful', 'vocal', '##izations', 'sons', 'pioneers']\n"]}]},{"cell_type":"markdown","source":["\n","Now we set the maximum sequence lengths for our training and test\n","sentences as `MAX_LEN_TRAIN` and `MAX_LEN_TEST`. The maximum length\n","supported by the used BERT model is 512.\n","\n","The token `[SEP]` is another special token required by BERT at the\n","end of the sentence."],"metadata":{"id":"iGEGsXZLThMZ"}},{"cell_type":"code","source":["MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n","\n","tokenized_train = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized_train]\n","tokenized_test  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_test]\n","tokenized_ood  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_ood]\n","\n","print (\"\\nThe truncated tokenized first training sentence:\")\n","print (tokenized_train[0])"],"metadata":{"id":"YxtWll1wThtk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"768f6622-a31f-47f9-a8b0-b4094dcbdc51","executionInfo":{"status":"ok","timestamp":1678689564111,"user_tz":-60,"elapsed":27,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","The truncated tokenized first training sentence:\n","['[CLS]', 'wildly', 'alive', 'SEP']\n"]}]},{"cell_type":"markdown","source":["\n","Next we use the BERT tokenizer to convert each token into an integer\n","index in the BERT vocabulary. We also pad any shorter sequences to\n","`MAX_LEN_TRAIN` or `MAX_LEN_TEST` indices with trailing zeros."],"metadata":{"id":"2FDmuiZ5VaOQ"}},{"cell_type":"code","source":["ids_train = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_train]\n","ids_train = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)),\n","                             mode='constant') for i in ids_train])\n","\n","ids_test = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_test]\n","ids_test = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)),\n","                            mode='constant') for i in ids_test])\n","\n","\n","ids_ood = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_ood]\n","ids_ood = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)),\n","                            mode='constant') for i in ids_ood])\n","\n","print (\"\\nThe indices of the first training sentence:\")\n","print (ids_train[0])"],"metadata":{"id":"H0LgugShVasR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"da4ba5e8-befb-4326-8732-f45ca5c25eec","executionInfo":{"status":"ok","timestamp":1678689567915,"user_tz":-60,"elapsed":3827,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","The indices of the first training sentence:\n","[  101 13544  4142   100     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0]\n"]}]},{"cell_type":"markdown","source":["BERT also requires *attention masks*, with 1 for each real token in\n","the sequences and 0 for the padding:"],"metadata":{"id":"KXHXkvs5JTet"}},{"cell_type":"code","source":["amasks_train, amasks_test , amasks_ood = [], [] , []\n","\n","for seq in ids_train:\n","  seq_mask = [float(i>0) for i in seq]\n","  amasks_train.append(seq_mask)\n","\n","for seq in ids_test:\n","  seq_mask = [float(i>0) for i in seq]\n","  amasks_test.append(seq_mask)\n","\n","\n","for seq in ids_ood:\n","  seq_mask = [float(i>0) for i in seq]\n","  amasks_ood.append(seq_mask)"],"metadata":{"id":"2yaiNP45Dkcm","executionInfo":{"status":"ok","timestamp":1678689575216,"user_tz":-60,"elapsed":7328,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["We use scikit-learn's train_test_split() to use 10% of our training\n","data as a validation set, and then convert all data into\n","torch.tensors."],"metadata":{"id":"Ko7iCp5cJiyd"}},{"cell_type":"code","source":["(train_inputs, validation_inputs,\n"," train_labels, validation_labels) = train_test_split(ids_train, labels_train,\n","                                                     random_state=42,\n","                                                     test_size=0.1)\n","(train_masks, validation_masks,\n"," _, _) = train_test_split(amasks_train, ids_train,\n","                          random_state=42, test_size=0.1)\n","\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks  = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks  = torch.tensor(validation_masks)\n","test_inputs = torch.tensor(ids_test)\n","test_labels = torch.tensor(labels_test)\n","test_masks  = torch.tensor(amasks_test)\n","ood_inputs = torch.tensor(ids_ood)\n","ood_labels = torch.tensor(labels_ood)\n","ood_masks  = torch.tensor(amasks_ood)\n","\n"],"metadata":{"id":"p3djaDPDJnZ9","executionInfo":{"status":"ok","timestamp":1678689575876,"user_tz":-60,"elapsed":667,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["Next we create PyTorch *DataLoader*s for all data sets.\n","For fine-tuning BERT on a specific task, the authors recommend a\n","batch size of 16 or 32."],"metadata":{"id":"a4TDX7VtJ2aA"}},{"cell_type":"code","source":["BATCH_SIZE = 16\n","\n","print('\\nDatasets:')\n","print('Train: ', end=\"\")\n","train_data = TensorDataset(train_inputs, train_masks,\n","                           train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler,\n","                              batch_size=BATCH_SIZE)\n","print(len(train_data), 'reviews')\n","\n","print('Validation: ', end=\"\")\n","validation_data = TensorDataset(validation_inputs, validation_masks,\n","                                validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data,\n","                                   sampler=validation_sampler,\n","                                   batch_size=BATCH_SIZE)\n","print(len(validation_data), 'reviews')\n","\n","print('Test: ', end=\"\")\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler,\n","                             batch_size=BATCH_SIZE)\n","print(len(test_data), 'reviews')\n","\n","\n","print('OOD: ', end=\"\")\n","ood_data = TensorDataset(ood_inputs, ood_masks, ood_labels)\n","ood_sampler = SequentialSampler(ood_data)\n","ood_dataloader = DataLoader(ood_data, sampler=ood_sampler,\n","                             batch_size=BATCH_SIZE)\n","print(len(ood_data), 'reviews')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AjgT4K4J2Fv","outputId":"c189bd76-f363-422f-955a-e2db4b562649","executionInfo":{"status":"ok","timestamp":1678689575877,"user_tz":-60,"elapsed":36,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Datasets:\n","Train: 49118 reviews\n","Validation: 5458 reviews\n","Test: 13645 reviews\n","OOD: 5457 reviews\n"]}]},{"cell_type":"markdown","source":["BERT MODEL INITIALIZATION\n","\n","We now load a pretrained BERT model with a single linear\n","classification layer added on top.\n"],"metadata":{"id":"a4Wi5wQqKEuc"}},{"cell_type":"code","source":["model = BertForSequenceClassification.from_pretrained(BERTMODEL,\n","                                                      num_labels=2,\n","                                                      output_hidden_states=True)\n","\n","\n","model.cuda()\n","print('\\nPretrained BERT model \"{}\" loaded'.format(BERTMODEL))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdIeyBLOJ2DE","outputId":"003e8873-4523-4905-ba44-ef39924e70ba","executionInfo":{"status":"ok","timestamp":1678689627482,"user_tz":-60,"elapsed":51634,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 433/433 [00:00<00:00, 209570.00B/s]\n","100%|██████████| 440473133/440473133 [00:36<00:00, 11998937.06B/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Pretrained BERT model \"bert-base-uncased\" loaded\n"]}]},{"cell_type":"markdown","source":["\n","We set the remaining hyperparameters needed for fine-tuning the\n","pretrained model: \n"," * EPOCHS: the number of training epochs in fine-tuning\n","   (recommended values between 2 and 4) \n"," * WEIGHT_DECAY: weight decay for the Adam optimizer \n"," * LR: learning rate for the Adam optimizer \n","   (2e-5 to 5e-5 recommended) \n"," * WARMUP_STEPS: number of warmup steps to (linearly) reach the\n","   set learning rate\n","\n"," We also need to grab the training parameters from the pretrained\n"," model."],"metadata":{"id":"ynTgfHzeLUXY"}},{"cell_type":"code","source":["EPOCHS = 4\n","WEIGHT_DECAY = 0.01\n","LR = 2e-5\n","WARMUP_STEPS =int(0.2*len(train_dataloader))\n","\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters()\n","                if not any(nd in n for nd in no_decay)],\n","     'weight_decay': WEIGHT_DECAY},\n","    {'params': [p for n, p in model.named_parameters()\n","                if any(nd in n for nd in no_decay)],\n","     'weight_decay': 0.0}\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n","scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS,\n","                                 t_total=len(train_dataloader)*EPOCHS)"],"metadata":{"id":"ZSPy9RhsJ2AT","executionInfo":{"status":"ok","timestamp":1678689627483,"user_tz":-60,"elapsed":23,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["LEARNING\n","\n","Let's now define functions to train() and evaluate() the model:"],"metadata":{"id":"zpTM0QR9Lyee"}},{"cell_type":"code","source":["def train(epoch, loss_vector=None, log_interval=200):\n","    # Set model to training mode\n","    model.train().to(device)\n","\n","    # Loop over each batch from the training set\n","    for step, batch in enumerate(train_dataloader):\n","        # Copy data to GPU if needed\n","        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Zero gradient buffers\n","        optimizer.zero_grad()\n","\n","        with torch.set_grad_enabled(True):\n","            # Forward pass\n","            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,\n","                         labels=b_labels)[0]\n","\n","        if loss_vector is not None:\n","            loss_vector.append(loss.item())\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update weights\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Clear unused variables\n","        del  b_input_mask, b_labels\n","\n","        if step % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                  epoch, step * len(b_input_ids), len(train_dataloader.dataset),\n","                  100. * step / len(train_dataloader), loss.item()))\n","            \n","    # Clear unused variables\n","    del b_input_ids,batch, loss\n"],"metadata":{"id":"XBbxjNth2BoE","executionInfo":{"status":"ok","timestamp":1678689627484,"user_tz":-60,"elapsed":23,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def evaluate(loader):\n","  model.eval()\n","\n","  n_correct, n_all = 0, 0\n","\n","  for batch in loader:\n","    batch = tuple(t.to(device) for t in batch)\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None,\n","                      attention_mask=b_input_mask)\n","      logits = outputs[0]\n","\n","    logits = logits.detach().cpu().numpy()\n","    predictions = np.argmax(logits, axis=1)\n","\n","    labels = b_labels.to('cpu').numpy()\n","    n_correct += np.sum(predictions == labels)\n","    n_all += len(labels)\n","\n","  print('Accuracy: [{}/{}] {:.4f}'.format(n_correct, n_all,\n","                                          n_correct/n_all))\n","\n","\n","\n","    "],"metadata":{"id":"6S9746L21fMw","executionInfo":{"status":"ok","timestamp":1678689627485,"user_tz":-60,"elapsed":23,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["Now we are ready to train our model using the train()\n","function. After each epoch, we evaluate the model using the\n","validation set and evaluate()."],"metadata":{"id":"FMuuXi6lL65Q"}},{"cell_type":"code","source":["train_lossv = []\n","for epoch in range(1, EPOCHS + 1):\n","    print()\n","    train(epoch, train_lossv)\n","    print('\\nValidation set:')\n","    evaluate(validation_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-TeGUafJ16j","outputId":"8a4c4620-3b06-4556-8bfa-7dc694b53bf8","executionInfo":{"status":"ok","timestamp":1678690844395,"user_tz":-60,"elapsed":1216933,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/49118 (0%)]\tLoss: 0.713533\n","Train Epoch: 1 [3200/49118 (7%)]\tLoss: 0.546485\n","Train Epoch: 1 [6400/49118 (13%)]\tLoss: 0.364176\n","Train Epoch: 1 [9600/49118 (20%)]\tLoss: 0.179698\n","Train Epoch: 1 [12800/49118 (26%)]\tLoss: 0.371124\n","Train Epoch: 1 [16000/49118 (33%)]\tLoss: 0.522638\n","Train Epoch: 1 [19200/49118 (39%)]\tLoss: 0.462169\n","Train Epoch: 1 [22400/49118 (46%)]\tLoss: 0.331721\n","Train Epoch: 1 [25600/49118 (52%)]\tLoss: 0.357517\n","Train Epoch: 1 [28800/49118 (59%)]\tLoss: 0.354009\n","Train Epoch: 1 [32000/49118 (65%)]\tLoss: 0.323098\n","Train Epoch: 1 [35200/49118 (72%)]\tLoss: 0.259602\n","Train Epoch: 1 [38400/49118 (78%)]\tLoss: 0.439180\n","Train Epoch: 1 [41600/49118 (85%)]\tLoss: 0.059927\n","Train Epoch: 1 [44800/49118 (91%)]\tLoss: 0.201739\n","Train Epoch: 1 [48000/49118 (98%)]\tLoss: 0.606802\n","\n","Validation set:\n","Accuracy: [4978/5458] 0.9121\n","\n","Train Epoch: 2 [0/49118 (0%)]\tLoss: 0.097868\n","Train Epoch: 2 [3200/49118 (7%)]\tLoss: 0.230181\n","Train Epoch: 2 [6400/49118 (13%)]\tLoss: 0.426867\n","Train Epoch: 2 [9600/49118 (20%)]\tLoss: 0.437561\n","Train Epoch: 2 [12800/49118 (26%)]\tLoss: 0.579413\n","Train Epoch: 2 [16000/49118 (33%)]\tLoss: 0.146929\n","Train Epoch: 2 [19200/49118 (39%)]\tLoss: 0.063354\n","Train Epoch: 2 [22400/49118 (46%)]\tLoss: 0.408025\n","Train Epoch: 2 [25600/49118 (52%)]\tLoss: 0.373530\n","Train Epoch: 2 [28800/49118 (59%)]\tLoss: 0.240557\n","Train Epoch: 2 [32000/49118 (65%)]\tLoss: 0.070803\n","Train Epoch: 2 [35200/49118 (72%)]\tLoss: 0.143388\n","Train Epoch: 2 [38400/49118 (78%)]\tLoss: 0.060071\n","Train Epoch: 2 [41600/49118 (85%)]\tLoss: 0.232317\n","Train Epoch: 2 [44800/49118 (91%)]\tLoss: 0.254449\n","Train Epoch: 2 [48000/49118 (98%)]\tLoss: 0.373108\n","\n","Validation set:\n","Accuracy: [5082/5458] 0.9311\n","\n","Train Epoch: 3 [0/49118 (0%)]\tLoss: 0.021402\n","Train Epoch: 3 [3200/49118 (7%)]\tLoss: 0.025294\n","Train Epoch: 3 [6400/49118 (13%)]\tLoss: 0.055069\n","Train Epoch: 3 [9600/49118 (20%)]\tLoss: 0.005327\n","Train Epoch: 3 [12800/49118 (26%)]\tLoss: 0.026997\n","Train Epoch: 3 [16000/49118 (33%)]\tLoss: 0.015794\n","Train Epoch: 3 [19200/49118 (39%)]\tLoss: 0.009980\n","Train Epoch: 3 [22400/49118 (46%)]\tLoss: 0.188299\n","Train Epoch: 3 [25600/49118 (52%)]\tLoss: 0.041169\n","Train Epoch: 3 [28800/49118 (59%)]\tLoss: 0.047152\n","Train Epoch: 3 [32000/49118 (65%)]\tLoss: 0.036689\n","Train Epoch: 3 [35200/49118 (72%)]\tLoss: 0.275102\n","Train Epoch: 3 [38400/49118 (78%)]\tLoss: 0.024004\n","Train Epoch: 3 [41600/49118 (85%)]\tLoss: 0.069528\n","Train Epoch: 3 [44800/49118 (91%)]\tLoss: 0.198987\n","Train Epoch: 3 [48000/49118 (98%)]\tLoss: 0.016316\n","\n","Validation set:\n","Accuracy: [5087/5458] 0.9320\n","\n","Train Epoch: 4 [0/49118 (0%)]\tLoss: 0.022773\n","Train Epoch: 4 [3200/49118 (7%)]\tLoss: 0.235955\n","Train Epoch: 4 [6400/49118 (13%)]\tLoss: 0.028377\n","Train Epoch: 4 [9600/49118 (20%)]\tLoss: 0.072153\n","Train Epoch: 4 [12800/49118 (26%)]\tLoss: 0.103743\n","Train Epoch: 4 [16000/49118 (33%)]\tLoss: 0.097837\n","Train Epoch: 4 [19200/49118 (39%)]\tLoss: 0.053805\n","Train Epoch: 4 [22400/49118 (46%)]\tLoss: 0.015838\n","Train Epoch: 4 [25600/49118 (52%)]\tLoss: 0.015724\n","Train Epoch: 4 [28800/49118 (59%)]\tLoss: 0.005819\n","Train Epoch: 4 [32000/49118 (65%)]\tLoss: 0.264018\n","Train Epoch: 4 [35200/49118 (72%)]\tLoss: 0.014030\n","Train Epoch: 4 [38400/49118 (78%)]\tLoss: 0.139646\n","Train Epoch: 4 [41600/49118 (85%)]\tLoss: 0.008628\n","Train Epoch: 4 [44800/49118 (91%)]\tLoss: 0.018277\n","Train Epoch: 4 [48000/49118 (98%)]\tLoss: 0.012213\n","\n","Validation set:\n","Accuracy: [5085/5458] 0.9317\n"]}]},{"cell_type":"markdown","source":["Let's take a look at our training loss over all batches:"],"metadata":{"id":"RbJJCz8GMDt1"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(15,8))\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(train_lossv, label='original')\n","plt.plot(np.convolve(train_lossv, np.ones(101), 'same') / 101,\n","         label='averaged')\n","plt.legend(loc='best')\n","plt.savefig(\"training-loss.png\")\n","plt.show()\n"],"metadata":{"id":"YehHezB-J13b","executionInfo":{"status":"ok","timestamp":1678690844398,"user_tz":-60,"elapsed":14,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["Inference\n","\n","For a better measure of the quality of the model, let's see the\n","model accuracy for the test reviews."],"metadata":{"id":"cZ60h-XMMQcK"}},{"cell_type":"code","source":["print('\\nTest set:')\n","evaluate(test_dataloader)\n","# eof"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OoW9yYglJ1vs","outputId":"04ac14b5-82a3-463d-dee6-52e7f5176f32","executionInfo":{"status":"ok","timestamp":1678690948112,"user_tz":-60,"elapsed":103724,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test set:\n","Accuracy: [12746/13645] 0.9341\n"]}]},{"cell_type":"markdown","source":["# OOD detection "],"metadata":{"id":"J5EOhawoaTit"}},{"cell_type":"markdown","source":["Extract BERT logits, features and predictions"],"metadata":{"id":"u_2e2L6da_xj"}},{"cell_type":"code","source":["def extract_bert_features(loader):\n","    model.eval()\n","\n","    last_layer_list = []\n","    logits_list = []\n","    softmax_list = []\n","    label_list = []\n","    pred_list = []\n","\n","    for batch in loader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask)\n","            last_layer = outputs[1][-1][:,0,:]\n","            logits = outputs[0]\n","            predictions = np.argmax(logits.detach().cpu().numpy(), axis=1)\n","            softmax = torch.nn.functional.softmax(logits, dim=-1)\n","\n","        last_layer_list.append(last_layer)\n","        logits_list.append(logits.cpu().numpy())\n","        softmax_list.append(softmax.cpu().numpy())\n","        label_list.append(b_labels.cpu().numpy())\n","        pred_list.append(predictions)\n","    \n","\n","        \n","\n","    last_layer = torch.cat(last_layer_list, dim=0)\n","    logits = np.concatenate(logits_list, axis=0)\n","    softmax = np.concatenate(softmax_list, axis=0)\n","    labels = np.concatenate(label_list, axis=0)\n","    predictions = np.concatenate(pred_list, axis=0)\n","\n","    return logits, last_layer.to('cpu').numpy(),  softmax, labels, predictions\n","    \n","\n"],"metadata":{"id":"1VuC8taNaTHb","executionInfo":{"status":"ok","timestamp":1678690948114,"user_tz":-60,"elapsed":68,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["Optimized memory version to be tested"],"metadata":{"id":"UomewBAh1HtH"}},{"cell_type":"code","source":["\"\"\"\n","\n","def extract_bert_features(loader):\n","    model.eval()\n","\n","    last_layer_list = []\n","    logits_list = []\n","    softmax_list = []\n","    label_list = []\n","    pred_list = []\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","\n","            outputs = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask)\n","            last_layer = outputs[-1]\n","            logits = outputs[0].detach().cpu().numpy()\n","            predictions = np.argmax(logits, axis=1)\n","            softmax = torch.nn.functional.softmax(outputs[0], dim=-1).cpu().numpy()\n","\n","            last_layer_list.append(last_layer.cpu())\n","            logits_list.append(logits)\n","            softmax_list.append(softmax)\n","            label_list.append(b_labels.detach().cpu().numpy())\n","            pred_list.append(predictions)\n","\n","    last_layer = torch.cat(last_layer_list, dim=0).numpy()\n","    labels = np.concatenate(label_list, axis=0)\n","    predictions = np.concatenate(pred_list, axis=0)\n","\n","    return last_layer.to('cpu').numpy(), logits_list, softmax_list, labels, predictions\n","\n","    \"\"\"\n"],"metadata":{"id":"PDIo_Jxxv03O","colab":{"base_uri":"https://localhost:8080/","height":226},"executionInfo":{"status":"ok","timestamp":1678690948115,"user_tz":-60,"elapsed":65,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}},"outputId":"1f30a5b8-9cb8-413f-d648-1c7b63aa3f5a"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n\\ndef extract_bert_features(loader):\\n    model.eval()\\n\\n    last_layer_list = []\\n    logits_list = []\\n    softmax_list = []\\n    label_list = []\\n    pred_list = []\\n\\n    with torch.no_grad():\\n        for batch in loader:\\n            batch = tuple(t.to(device) for t in batch)\\n            b_input_ids, b_input_mask, b_labels = batch\\n\\n            outputs = model(b_input_ids, token_type_ids=None,\\n                            attention_mask=b_input_mask)\\n            last_layer = outputs[-1]\\n            logits = outputs[0].detach().cpu().numpy()\\n            predictions = np.argmax(logits, axis=1)\\n            softmax = torch.nn.functional.softmax(outputs[0], dim=-1).cpu().numpy()\\n\\n            last_layer_list.append(last_layer.cpu())\\n            logits_list.append(logits)\\n            softmax_list.append(softmax)\\n            label_list.append(b_labels.detach().cpu().numpy())\\n            pred_list.append(predictions)\\n\\n    last_layer = torch.cat(last_layer_list, dim=0).numpy()\\n    labels = np.concatenate(label_list, axis=0)\\n    predictions = np.concatenate(pred_list, axis=0)\\n\\n    return last_layer.to('cpu').numpy(), logits_list, softmax_list, labels, predictions\\n\\n    \""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["Now applying on the data sets"],"metadata":{"id":"2kS7LfTRMknK"}},{"cell_type":"code","source":["train_features = extract_bert_features(train_dataloader)\n","test_features = extract_bert_features(test_dataloader)\n","ood_features = extract_bert_features(ood_dataloader)"],"metadata":{"id":"p0tBrqYkbDJk","executionInfo":{"status":"ok","timestamp":1678691183255,"user_tz":-60,"elapsed":235202,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["## OOD with Mahalanobis distance using last layer logits and features"],"metadata":{"id":"DnC_LAB21srU"}},{"cell_type":"code","source":["#define the function that calculates the metrics \n","def metrics(scores: np.ndarray, labels: np.ndarray, threshold: float):\n","    \"\"\"Computes the number of\n","        * true positives,\n","        * false positives,\n","        * true negatives,\n","        * false negatives,\n","        * the accuracy\n","        * false positive rate\n","        * taux d'erreur\n","    for a given threshold\n","\n","    \"\"\"\n","    pos = np.where(scores >= threshold) \n","    neg = np.where(scores < threshold)\n","    n_pos = len(pos[0])\n","    n_neg = len(neg[0])\n","\n","    tp = np.sum(labels[pos])\n","    fp = n_pos - tp\n","    fn = np.sum(labels[neg])\n","    tn = n_neg - fn\n","\n","    FPR = fp / (fp + tn)\n","    Accuracy = (tp+tn)/len(scores)\n","    ERR = 1 - Accuracy\n","    return FPR, ERR"],"metadata":{"id":"mVLLg9QQBFpv","executionInfo":{"status":"ok","timestamp":1678691183256,"user_tz":-60,"elapsed":13,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["Covariance matrix for the latent features of the internal distribution (SST2)"],"metadata":{"id":"Ainp69jYM75A"}},{"cell_type":"code","source":["#mean, Covariance matrix and inverse for the latent features of the internal distribution (SST2) \n","                  #for logits#                          #for features#\n","train_mean = [np.mean(train_features[0], axis=0)  ,np.mean(train_features[1], axis=0)]\n","\n","                  #for logits#                           #for features#\n","train_cov = [np.cov(train_features[0], rowvar=False), np.cov(train_features[1], rowvar=False)] \n","\n","                   #for logits#                          #for features#\n","train_inv_cov = [np.linalg.inv(train_cov[0]), np.linalg.inv(train_cov[1])] \n"],"metadata":{"id":"je8mMk_GCyBC","executionInfo":{"status":"ok","timestamp":1678691183257,"user_tz":-60,"elapsed":12,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["for i in [0,1]:\n"," \n","  #Calculation of the Mahalanobis distance on the in-ds and ood data\n","  inds_test_scores = []\n","  ood_test_scores = []\n","\n","  for feature in test_features[i]:\n","    score = mahalanobis(feature, train_mean[i], train_inv_cov[i])\n","    inds_test_scores.append(score)\n","\n","  for feature in ood_features[i]:\n","    score = mahalanobis(feature, train_mean[i], train_inv_cov[i])\n","    ood_test_scores.append(score)\n","\n","  #We will evaluate the performance of OOD detection using the AUROC,\n","  # AUPR, FPR and ERR metrics\n","  \n","  labels = np.concatenate([np.zeros(len(inds_test_scores)), np.ones(len(ood_test_scores))])\n","  scores = np.concatenate([inds_test_scores, ood_test_scores])\n","\n","  #define the threshold for ood detection\n","  threshold = np.mean(inds_test_scores) +  np.std(inds_test_scores)\n","\n","  #Calculation of metrics\n","  if i == 0:\n","    use = 'logits'\n","  else:\n","      use = 'features'\n","  FPR, ERR = metrics (scores, labels, threshold)\n","  auroc = roc_auc_score(labels, scores)\n","  aupr = average_precision_score(labels, scores)\n","  \n","  print(\"metrics in ood detection with Mahalanibos distance using \"+use+\" of last layer\")\n","  print('AUROC:', auroc)\n","  print('AUPR:', aupr)\n","  print('FPR:', FPR)\n","  print('ERR:', ERR)\n","\n","  #plot of auroc curve and aupr curve\n","  fpr, tpr, _ = roc_curve(labels, scores)\n","  precision, recall, _ = precision_recall_curve(labels, scores)\n","\n","  plt.figure(figsize=(10, 5))\n","  plt.subplot(121)\n","  plt.plot(fpr, tpr)\n","  plt.plot([0, 1], [0, 1], linestyle='--')\n","  plt.xlabel('False Positive Rate')\n","  plt.ylabel('True Positive Rate')\n","  plt.title('AUROC Curve')\n","\n","  plt.subplot(122)\n","  plt.plot(recall, precision)\n","  plt.xlabel('Recall')\n","  plt.ylabel('Precision')\n","  plt.title('AUPR Curve')\n","\n","  plt.tight_layout()\n","  plt.savefig(\"auroc_aupr_mahalabinos_\"+use+\".png\")\n","  \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LfI--kWMBsKG","outputId":"0e2dd558-808a-4e7f-978f-82de6fd8a0b1","executionInfo":{"status":"ok","timestamp":1678691185460,"user_tz":-60,"elapsed":2213,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["metrics in ood detection with Mahalanibos distance using logits of last layer\n","AUROC: 0.7055443225704168\n","AUPR: 0.46884866794615737\n","FPR: 0.14188347379992672\n","ERR: 0.26855826615014133\n","metrics in ood detection with Mahalanibos distance using features of last layer\n","AUROC: 0.8602321638785204\n","AUPR: 0.6947067587917335\n","FPR: 0.1546353975815317\n","ERR: 0.19270233483404875\n"]}]},{"cell_type":"markdown","source":["## OOD detection using Energy-based score\n"],"metadata":{"id":"5VNjfXwq2OJN"}},{"cell_type":"code","source":[],"metadata":{"id":"tkRz5Ima2Oeo","executionInfo":{"status":"ok","timestamp":1678691185461,"user_tz":-60,"elapsed":41,"user":{"displayName":"Aurelie NGALULA","userId":"12113917455542123315"}}},"execution_count":34,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}