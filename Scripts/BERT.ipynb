{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QqJbxx-_BU7"
      },
      "source": [
        "#Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fy54k2O-Q3N"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch torchvision\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install datasets\n",
        "!pip install pytorch_transformers\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J13iSqp__qMw"
      },
      "source": [
        "Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-shbksg4Wz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f1e3ac-4808-4e57-90a5-ae66413e32c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version: 1.13.1+cu116 Device: cuda [NVIDIA A100-SXM4-40GB]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import (TensorDataset, DataLoader,\n",
        "                              RandomSampler, SequentialSampler)\n",
        "\n",
        "from pytorch_transformers import BertTokenizer, BertConfig\n",
        "from pytorch_transformers import BertForSequenceClassification\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
        "\n",
        "from distutils.version import LooseVersion as LV\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
        "import torch.nn.functional as F\n",
        "import io\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "sns.set()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    devicename = '['+torch.cuda.get_device_name(0)+']'\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    devicename = \"\"\n",
        "    \n",
        "print('Using PyTorch version:', torch.__version__,\n",
        "      'Device:', device, devicename)\n",
        "assert(LV(torch.__version__) >= LV(\"1.0.0\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the IMDb and SST-2 datasets and extract them."
      ],
      "metadata": {
        "id": "PXhFhTFsYSGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "\n",
        "# Load the SST-2 dataset\n",
        "sst2_dataset = load_dataset(\"glue\", \"sst2\")"
      ],
      "metadata": {
        "id": "S2w3DXq8zmL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the IMDb dataset using pandas, and preprocess the text data by removing HTML tags, non-alphanumeric characters, and stop words."
      ],
      "metadata": {
        "id": "Iw5Lk4nFXf1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(imdb_dataset.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoEDXTDQr89F",
        "outputId": "49c60956-a6e6-4eaf-98b9-01ea713e72ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': ['text', 'label'], 'test': ['text', 'label'], 'unsupervised': ['text', 'label']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDb dataset\n",
        "imdb_df = pd.concat([pd.DataFrame(imdb_dataset['train']),pd.DataFrame(imdb_dataset['test'])])\n",
        "imdb_df = imdb_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print('\\nIMDB data loaded:')\n",
        "print('data set:', imdb_df.shape)\n",
        "print(imdb_df['label'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXioOoLXj5_6",
        "outputId": "d46dc73c-1dc5-4b21-d55f-da81ee7aa356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IMDB data loaded:\n",
            "data set: (50000, 2)\n",
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the SST-2 dataset using pandas, and preprocess the text data in the same way as the IMDb dataset"
      ],
      "metadata": {
        "id": "M5ajmdUSTPKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sst2_dataset.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPKsg4shr4zn",
        "outputId": "f6b54ed0-1ee7-4dc0-be76-ee5b6c61eb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': ['sentence', 'label', 'idx'], 'validation': ['sentence', 'label', 'idx'], 'test': ['sentence', 'label', 'idx']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SST-2 dataset\n",
        "\n",
        "sst2_df = pd.concat([pd.DataFrame(sst2_dataset['train'])[['sentence', 'label']],pd.DataFrame(sst2_dataset['validation'])[['sentence', 'label']]])\n",
        "sst2_df = sst2_df.rename(columns={'sentence': 'text'})\n",
        "sst2_df = sst2_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print('\\nSST2 data loaded:')\n",
        "print('data set:', sst2_df.shape)\n",
        "print(sst2_df['label'].unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sHU-_OllRtQ",
        "outputId": "4b4e59b0-b36a-4346-f04a-47ceeb5a8190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SST2 data loaded:\n",
            "data set: (68221, 2)\n",
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text data\n",
        "sst2_df ['text'] = sst2_df ['text'].str.replace('<.*?>', '', regex=True) # remove HTML tags\n",
        "sst2_df ['text'] = sst2_df ['text'].str.replace('[^a-zA-Z0-9\\s]', '', regex=True) # remove non-alphanumeric characters\n",
        "stop_words = set(stopwords.words('english'))\n",
        "sst2_df ['text'] = sst2_df ['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words])) # remove stop words"
      ],
      "metadata": {
        "id": "EkwE9ffRA5YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's view some random reviews:\n",
        "print(sst2_df.sample(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ2PQoK3BMxC",
        "outputId": "01ce9f4e-3508-4204-b2dd-e9b535f1c289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label\n",
            "27055           semiamusing watch robert deniro belt jet      1\n",
            "15827                                            tatters      0\n",
            "26356  give filmmaker unlimited amount phony blood no...      0\n",
            "60486                   quiet introspective entertaining      1\n",
            "52295  sometimes felt though tiny two seater plane ca...      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train and test set"
      ],
      "metadata": {
        "id": "KSS15bBzr0lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your features and target variable\n",
        "X = sst2_df.drop(\"label\", axis=1)\n",
        "y = sst2_df[\"label\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shape of the train and test sets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "sst2_train_df = pd.concat([X_train,y_train], axis=1)\n",
        "sst2_test_df = pd.concat([X_test,y_test], axis=1)\n",
        "\n",
        "print('\\nSST2 data re splitted:')\n",
        "print('train:', sst2_train_df.shape)\n",
        "print('test:', sst2_test_df.shape)\n",
        "print(sst2_train_df['label'].unique())\n",
        "print(sst2_test_df['label'].unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trDNhh0nsDGL",
        "outputId": "fb87deb3-20cd-4b19-f462-b40b026c50b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (54576, 1)\n",
            "X_test shape: (13645, 1)\n",
            "y_train shape: (54576,)\n",
            "y_test shape: (13645,)\n",
            "\n",
            "SST2 data re splitted:\n",
            "train: (54576, 2)\n",
            "test: (13645, 2)\n",
            "[1 0]\n",
            "[1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's view some random reviews:\n",
        "print(sst2_train_df.sample(5))\n",
        "print(sst2_test_df.sample(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnTy7rj631jm",
        "outputId": "ddc44362-4045-4cd5-e357-0b1c8f24eefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  text  label\n",
            "43351                           cancer      0\n",
            "49166                  fails endeavors      0\n",
            "29000                  squirming seats      0\n",
            "32782                   ambitious film      1\n",
            "62492  perfectly rendered period piece      1\n",
            "                                                    text  label\n",
            "61153                                          lousy one      0\n",
            "13856                                    already obscure      0\n",
            "13274                                    selfdeprecating      0\n",
            "40494                     nonsensical laughable plotting      0\n",
            "20367  repugnant adaptation classic text since roland...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IN-DS: SST2\n",
        "OOD-DS: IMDB"
      ],
      "metadata": {
        "id": "KnVplK7CuFem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = sst2_train_df\n",
        "test_df = sst2_test_df\n",
        "n_ood = 0.2\n",
        "ood_df = imdb_df.sample(int(n_ood*test_df.shape[0]))\n",
        "ood_df = ood_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "stJtXnEyuE5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text data\n",
        "ood_df ['text'] = ood_df ['text'].str.replace('<.*?>', '', regex=True) # remove HTML tags\n",
        "ood_df ['text'] = ood_df ['text'].str.replace('[^a-zA-Z0-9\\s]', '', regex=True) # remove non-alphanumeric characters\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ood_df ['text'] = ood_df ['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words])) # remove stop words\n",
        "\n",
        "\n",
        "# Let's view some random reviews:\n",
        "print(ood_df.sample(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbFj52jjon7X",
        "outputId": "b3f93018-65a1-42d2-a54b-cd8156b96ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text  label\n",
            "3419  I remember watching child UK mesmerized story ...      1\n",
            "3352  Undoubtedly funniest movie I ever seen Its def...      1\n",
            "5290  Cowboys James Stewart Walter Brennan take herd...      1\n",
            "2446  Such BS movie Its stupid antiRussian propagand...      0\n",
            "3481  Sequels hardly ever live original This definit...      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "fd4YYYJ2-OF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The token `[CLS]` is a special token required by BERT at the beginning of the sentence."
      ],
      "metadata": {
        "id": "1ZuRWQHKTbVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train = train_df.text.values\n",
        "sentences_train = [\"[CLS] \" + s for s in sentences_train]\n",
        "\n",
        "sentences_test = test_df.text.values\n",
        "sentences_test = [\"[CLS] \" + s for s in sentences_test]\n",
        "\n",
        "sentences_ood = ood_df.text.values\n",
        "sentences_ood = [\"[CLS] \" + s for s in sentences_ood]\n",
        "\n",
        "\n",
        "labels_train = train_df.label.values\n",
        "labels_test  = test_df.label.values\n",
        "labels_ood  = ood_df.label.values\n",
        "\n",
        "print (\"\\nThe first training sentence:\")\n",
        "print(sentences_train[0], 'LABEL:', labels_train[0])\n"
      ],
      "metadata": {
        "id": "UPKOd23EMA5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a597434-0628-4470-ef3c-2e260fa93b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The first training sentence:\n",
            "[CLS] wildly alive LABEL: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we use the BERT tokenizer to convert the sentences into tokens\n",
        "that match the data BERT was trained on.\n"
      ],
      "metadata": {
        "id": "71yBR1tYTeFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BERTMODEL = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(BERTMODEL,\n",
        "                                          do_lower_case=True)\n",
        "\n",
        "tokenized_train = [tokenizer.tokenize(s) for s in sentences_train]\n",
        "tokenized_test  = [tokenizer.tokenize(s) for s in sentences_test]\n",
        "tokenized_ood  = [tokenizer.tokenize(s) for s in sentences_ood]\n",
        "\n",
        "print (\"\\nThe full tokenized first training sentence:\")\n",
        "print (tokenized_train[0])\n",
        "\n",
        "print (\"\\nThe full tokenized first test sentence:\")\n",
        "print (tokenized_test[0])\n",
        "\n",
        "print (\"\\nThe full tokenized first OOD sentence:\")\n",
        "print (tokenized_ood[0])"
      ],
      "metadata": {
        "id": "_pTzOWtWTe0w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "57736ad7-17a8-4243-bb4f-3bc8b6d3a5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 2528144.85B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The full tokenized first training sentence:\n",
            "['[CLS]', 'wildly', 'alive']\n",
            "\n",
            "The full tokenized first test sentence:\n",
            "['[CLS]', 'best', 'script']\n",
            "\n",
            "The full tokenized first OOD sentence:\n",
            "['[CLS]', 'to', 'review', 'may', 'contain', 'spoil', '##ers', 'i', 'like', 'watching', 'movies', 'no', 'idea', 'going', 'happen', 'therefore', 'i', 'think', 'many', 'reviews', 'movie', 'contain', 'spoil', '##ers', '##i', 'watched', 'movie', 'i', 'must', 'rei', '##tera', '##te', 'best', 'ending', 'movie', 'ever', 'ever', 'ever', 'the', 'real', 'translation', 'the', 'beating', 'butterfly', '##s', 'wings', 'oddly', 'used', 'translated', 'title', 'i', 'suppose', 'thought', 'americans', 'wouldn', '##t', 'know', 'chaos', 'theory', 'except', 'saw', 'read', 'jurassic', 'park', 'the', 'movie', 'based', 'chaos', 'theory', 'one', 'small', 'event', 'affect', 'outcome', 'seemingly', 'unrelated', 'events', 'lead', 'back', 'one', 'event', 'the', 'movie', 'w', '##hir', '##l', '##wind', 'won', '##dro', '##us', 'cause', 'effect', 'follow', 'chain', 'chaos', 'inter', '##t', '##wine', '##s', 'several', 'characters', '20', 'in', 'way', 'ending', 'seems', 'inevitable', 'despite', 'think', 'perfect', 'ending', 'think', 'else', 'needed', 'said', 'it', 'time', 'brave', 'ending', 'too', 'bad', 'go', 'overseas', 'gem', 'like', 'one', 'ending', 'like', 'would', 'never', 'come', 'hollywood']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now we set the maximum sequence lengths for our training and test\n",
        "sentences as `MAX_LEN_TRAIN` and `MAX_LEN_TEST`. The maximum length\n",
        "supported by the used BERT model is 512.\n",
        "\n",
        "The token `[SEP]` is another special token required by BERT at the\n",
        "end of the sentence."
      ],
      "metadata": {
        "id": "iGEGsXZLThMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
        "\n",
        "tokenized_train = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized_train]\n",
        "tokenized_test  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_test]\n",
        "tokenized_ood  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_ood]\n",
        "\n",
        "print (\"\\nThe truncated tokenized first training sentence:\")\n",
        "print (tokenized_train[0])"
      ],
      "metadata": {
        "id": "YxtWll1wThtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f5a9c5-4e55-4ba0-cabc-6568f093027d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The truncated tokenized first training sentence:\n",
            "['[CLS]', 'wildly', 'alive', 'SEP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Next we use the BERT tokenizer to convert each token into an integer\n",
        "index in the BERT vocabulary. We also pad any shorter sequences to\n",
        "`MAX_LEN_TRAIN` or `MAX_LEN_TEST` indices with trailing zeros."
      ],
      "metadata": {
        "id": "2FDmuiZ5VaOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_train = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_train]\n",
        "ids_train = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)),\n",
        "                             mode='constant') for i in ids_train])\n",
        "\n",
        "ids_test = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_test]\n",
        "ids_test = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)),\n",
        "                            mode='constant') for i in ids_test])\n",
        "\n",
        "\n",
        "ids_ood = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_ood]\n",
        "ids_ood = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)),\n",
        "                            mode='constant') for i in ids_ood])\n",
        "\n",
        "print (\"\\nThe indices of the first training sentence:\")\n",
        "print (ids_train[0])"
      ],
      "metadata": {
        "id": "H0LgugShVasR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b734ae5f-ff58-4ab0-d667-edfeb1ea2f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The indices of the first training sentence:\n",
            "[  101 13544  4142   100     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT also requires *attention masks*, with 1 for each real token in\n",
        "the sequences and 0 for the padding:"
      ],
      "metadata": {
        "id": "KXHXkvs5JTet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amasks_train, amasks_test , amasks_ood = [], [] , []\n",
        "\n",
        "for seq in ids_train:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  amasks_train.append(seq_mask)\n",
        "\n",
        "for seq in ids_test:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  amasks_test.append(seq_mask)\n",
        "\n",
        "\n",
        "for seq in ids_ood:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  amasks_ood.append(seq_mask)"
      ],
      "metadata": {
        "id": "2yaiNP45Dkcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use scikit-learn's train_test_split() to use 10% of our training\n",
        "data as a validation set, and then convert all data into\n",
        "torch.tensors."
      ],
      "metadata": {
        "id": "Ko7iCp5cJiyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_inputs, validation_inputs,\n",
        " train_labels, validation_labels) = train_test_split(ids_train, labels_train,\n",
        "                                                     random_state=42,\n",
        "                                                     test_size=0.1)\n",
        "(train_masks, validation_masks,\n",
        " _, _) = train_test_split(amasks_train, ids_train,\n",
        "                          random_state=42, test_size=0.1)\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks  = torch.tensor(train_masks)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks  = torch.tensor(validation_masks)\n",
        "test_inputs = torch.tensor(ids_test)\n",
        "test_labels = torch.tensor(labels_test)\n",
        "test_masks  = torch.tensor(amasks_test)\n",
        "ood_inputs = torch.tensor(ids_ood)\n",
        "ood_labels = torch.tensor(labels_ood)\n",
        "ood_masks  = torch.tensor(amasks_ood)\n",
        "\n"
      ],
      "metadata": {
        "id": "p3djaDPDJnZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we create PyTorch *DataLoader*s for all data sets.\n",
        "For fine-tuning BERT on a specific task, the authors recommend a\n",
        "batch size of 16 or 32."
      ],
      "metadata": {
        "id": "a4TDX7VtJ2aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "print('\\nDatasets:')\n",
        "print('Train: ', end=\"\")\n",
        "train_data = TensorDataset(train_inputs, train_masks,\n",
        "                           train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler,\n",
        "                              batch_size=BATCH_SIZE)\n",
        "print(len(train_data), 'reviews')\n",
        "\n",
        "print('Validation: ', end=\"\")\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks,\n",
        "                                validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data,\n",
        "                                   sampler=validation_sampler,\n",
        "                                   batch_size=BATCH_SIZE)\n",
        "print(len(validation_data), 'reviews')\n",
        "\n",
        "print('Test: ', end=\"\")\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler,\n",
        "                             batch_size=BATCH_SIZE)\n",
        "print(len(test_data), 'reviews')\n",
        "\n",
        "\n",
        "print('OOD: ', end=\"\")\n",
        "ood_data = TensorDataset(ood_inputs, ood_masks, ood_labels)\n",
        "ood_sampler = SequentialSampler(ood_data)\n",
        "ood_dataloader = DataLoader(ood_data, sampler=ood_sampler,\n",
        "                             batch_size=BATCH_SIZE)\n",
        "print(len(ood_data), 'reviews')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AjgT4K4J2Fv",
        "outputId": "08645a78-7f6b-44d7-955d-3214bb09da60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Datasets:\n",
            "Train: 49118 reviews\n",
            "Validation: 5458 reviews\n",
            "Test: 13645 reviews\n",
            "OOD: 5457 reviews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT MODEL INITIALIZATION\n",
        "\n",
        "We now load a pretrained BERT model with a single linear\n",
        "classification layer added on top.\n"
      ],
      "metadata": {
        "id": "a4Wi5wQqKEuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(BERTMODEL,\n",
        "                                                      num_labels=2,\n",
        "                                                      output_hidden_states=True)\n",
        "\n",
        "\n",
        "model.cuda()\n",
        "print('\\nPretrained BERT model \"{}\" loaded'.format(BERTMODEL))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdIeyBLOJ2DE",
        "outputId": "e6bde486-d761-4857-ca8b-1366a280e7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 433/433 [00:00<00:00, 178174.59B/s]\n",
            "100%|██████████| 440473133/440473133 [00:05<00:00, 82095716.32B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pretrained BERT model \"bert-base-uncased\" loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We set the remaining hyperparameters needed for fine-tuning the\n",
        "pretrained model: \n",
        " * EPOCHS: the number of training epochs in fine-tuning\n",
        "   (recommended values between 2 and 4) \n",
        " * WEIGHT_DECAY: weight decay for the Adam optimizer \n",
        " * LR: learning rate for the Adam optimizer \n",
        "   (2e-5 to 5e-5 recommended) \n",
        " * WARMUP_STEPS: number of warmup steps to (linearly) reach the\n",
        "   set learning rate\n",
        "\n",
        " We also need to grab the training parameters from the pretrained\n",
        " model."
      ],
      "metadata": {
        "id": "ynTgfHzeLUXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 4\n",
        "WEIGHT_DECAY = 0.01\n",
        "LR = 2e-5\n",
        "WARMUP_STEPS =int(0.2*len(train_dataloader))\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters()\n",
        "                if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay': WEIGHT_DECAY},\n",
        "    {'params': [p for n, p in model.named_parameters()\n",
        "                if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n",
        "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS,\n",
        "                                 t_total=len(train_dataloader)*EPOCHS)"
      ],
      "metadata": {
        "id": "ZSPy9RhsJ2AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEARNING\n",
        "\n",
        "Let's now define functions to train() and evaluate() the model:"
      ],
      "metadata": {
        "id": "zpTM0QR9Lyee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, loss_vector=None, log_interval=200):\n",
        "    # Set model to training mode\n",
        "    model.train().to(device)\n",
        "\n",
        "    # Loop over each batch from the training set\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Copy data to GPU if needed\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Zero gradient buffers\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            # Forward pass\n",
        "            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,\n",
        "                         labels=b_labels)[0]\n",
        "\n",
        "        if loss_vector is not None:\n",
        "            loss_vector.append(loss.item())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Clear unused variables\n",
        "        del  b_input_mask, b_labels\n",
        "\n",
        "        if step % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, step * len(b_input_ids), len(train_dataloader.dataset),\n",
        "                  100. * step / len(train_dataloader), loss.item()))\n",
        "            \n",
        "    # Clear unused variables\n",
        "    del b_input_ids,batch, loss\n"
      ],
      "metadata": {
        "id": "XBbxjNth2BoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(loader):\n",
        "  model.eval()\n",
        "\n",
        "  n_correct, n_all = 0, 0\n",
        "\n",
        "  for batch in loader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "      logits = outputs[0]\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "    labels = b_labels.to('cpu').numpy()\n",
        "    n_correct += np.sum(predictions == labels)\n",
        "    n_all += len(labels)\n",
        "\n",
        "  print('Accuracy: [{}/{}] {:.4f}'.format(n_correct, n_all,\n",
        "                                          n_correct/n_all))\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "6S9746L21fMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train our model using the train()\n",
        "function. After each epoch, we evaluate the model using the\n",
        "validation set and evaluate()."
      ],
      "metadata": {
        "id": "FMuuXi6lL65Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_lossv = []\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print()\n",
        "    train(epoch, train_lossv)\n",
        "    print('\\nValidation set:')\n",
        "    evaluate(validation_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-TeGUafJ16j",
        "outputId": "d6c49317-3865-4162-e00b-def616ded7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/49118 (0%)]\tLoss: 0.680682\n",
            "Train Epoch: 1 [3200/49118 (7%)]\tLoss: 0.734865\n",
            "Train Epoch: 1 [6400/49118 (13%)]\tLoss: 0.396475\n",
            "Train Epoch: 1 [9600/49118 (20%)]\tLoss: 0.241096\n",
            "Train Epoch: 1 [12800/49118 (26%)]\tLoss: 0.426279\n",
            "Train Epoch: 1 [16000/49118 (33%)]\tLoss: 0.097280\n",
            "Train Epoch: 1 [19200/49118 (39%)]\tLoss: 0.696036\n",
            "Train Epoch: 1 [22400/49118 (46%)]\tLoss: 0.856345\n",
            "Train Epoch: 1 [25600/49118 (52%)]\tLoss: 0.213679\n",
            "Train Epoch: 1 [28800/49118 (59%)]\tLoss: 0.226035\n",
            "Train Epoch: 1 [32000/49118 (65%)]\tLoss: 0.159601\n",
            "Train Epoch: 1 [35200/49118 (72%)]\tLoss: 0.410396\n",
            "Train Epoch: 1 [38400/49118 (78%)]\tLoss: 0.456776\n",
            "Train Epoch: 1 [41600/49118 (85%)]\tLoss: 0.295818\n",
            "Train Epoch: 1 [44800/49118 (91%)]\tLoss: 0.125046\n",
            "Train Epoch: 1 [48000/49118 (98%)]\tLoss: 0.079889\n",
            "\n",
            "Validation set:\n",
            "Accuracy: [4995/5458] 0.9152\n",
            "\n",
            "Train Epoch: 2 [0/49118 (0%)]\tLoss: 0.064814\n",
            "Train Epoch: 2 [3200/49118 (7%)]\tLoss: 0.341254\n",
            "Train Epoch: 2 [6400/49118 (13%)]\tLoss: 0.173692\n",
            "Train Epoch: 2 [9600/49118 (20%)]\tLoss: 0.025104\n",
            "Train Epoch: 2 [12800/49118 (26%)]\tLoss: 0.036961\n",
            "Train Epoch: 2 [16000/49118 (33%)]\tLoss: 0.024722\n",
            "Train Epoch: 2 [19200/49118 (39%)]\tLoss: 0.140988\n",
            "Train Epoch: 2 [22400/49118 (46%)]\tLoss: 0.119160\n",
            "Train Epoch: 2 [25600/49118 (52%)]\tLoss: 0.189708\n",
            "Train Epoch: 2 [28800/49118 (59%)]\tLoss: 0.214930\n",
            "Train Epoch: 2 [32000/49118 (65%)]\tLoss: 0.072608\n",
            "Train Epoch: 2 [35200/49118 (72%)]\tLoss: 0.094079\n",
            "Train Epoch: 2 [38400/49118 (78%)]\tLoss: 0.415890\n",
            "Train Epoch: 2 [41600/49118 (85%)]\tLoss: 0.071080\n",
            "Train Epoch: 2 [44800/49118 (91%)]\tLoss: 0.132231\n",
            "Train Epoch: 2 [48000/49118 (98%)]\tLoss: 0.059166\n",
            "\n",
            "Validation set:\n",
            "Accuracy: [5068/5458] 0.9285\n",
            "\n",
            "Train Epoch: 3 [0/49118 (0%)]\tLoss: 0.072088\n",
            "Train Epoch: 3 [3200/49118 (7%)]\tLoss: 0.221831\n",
            "Train Epoch: 3 [6400/49118 (13%)]\tLoss: 0.279294\n",
            "Train Epoch: 3 [9600/49118 (20%)]\tLoss: 0.370646\n",
            "Train Epoch: 3 [12800/49118 (26%)]\tLoss: 0.026546\n",
            "Train Epoch: 3 [16000/49118 (33%)]\tLoss: 0.020416\n",
            "Train Epoch: 3 [19200/49118 (39%)]\tLoss: 0.116233\n",
            "Train Epoch: 3 [22400/49118 (46%)]\tLoss: 0.110258\n",
            "Train Epoch: 3 [25600/49118 (52%)]\tLoss: 0.420457\n",
            "Train Epoch: 3 [28800/49118 (59%)]\tLoss: 0.090247\n",
            "Train Epoch: 3 [32000/49118 (65%)]\tLoss: 0.031046\n",
            "Train Epoch: 3 [35200/49118 (72%)]\tLoss: 0.009588\n",
            "Train Epoch: 3 [38400/49118 (78%)]\tLoss: 0.246648\n",
            "Train Epoch: 3 [41600/49118 (85%)]\tLoss: 0.365557\n",
            "Train Epoch: 3 [44800/49118 (91%)]\tLoss: 0.165126\n",
            "Train Epoch: 3 [48000/49118 (98%)]\tLoss: 0.543623\n",
            "\n",
            "Validation set:\n",
            "Accuracy: [5081/5458] 0.9309\n",
            "\n",
            "Train Epoch: 4 [0/49118 (0%)]\tLoss: 0.093577\n",
            "Train Epoch: 4 [3200/49118 (7%)]\tLoss: 0.057997\n",
            "Train Epoch: 4 [6400/49118 (13%)]\tLoss: 0.146160\n",
            "Train Epoch: 4 [9600/49118 (20%)]\tLoss: 0.065473\n",
            "Train Epoch: 4 [12800/49118 (26%)]\tLoss: 0.008433\n",
            "Train Epoch: 4 [16000/49118 (33%)]\tLoss: 0.422452\n",
            "Train Epoch: 4 [19200/49118 (39%)]\tLoss: 0.013656\n",
            "Train Epoch: 4 [22400/49118 (46%)]\tLoss: 0.009715\n",
            "Train Epoch: 4 [25600/49118 (52%)]\tLoss: 0.019985\n",
            "Train Epoch: 4 [28800/49118 (59%)]\tLoss: 0.034605\n",
            "Train Epoch: 4 [32000/49118 (65%)]\tLoss: 0.008874\n",
            "Train Epoch: 4 [35200/49118 (72%)]\tLoss: 0.014868\n",
            "Train Epoch: 4 [38400/49118 (78%)]\tLoss: 0.582823\n",
            "Train Epoch: 4 [41600/49118 (85%)]\tLoss: 0.179267\n",
            "Train Epoch: 4 [44800/49118 (91%)]\tLoss: 0.039874\n",
            "Train Epoch: 4 [48000/49118 (98%)]\tLoss: 0.050465\n",
            "\n",
            "Validation set:\n",
            "Accuracy: [5086/5458] 0.9318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at our training loss over all batches:"
      ],
      "metadata": {
        "id": "RbJJCz8GMDt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_lossv, label='original')\n",
        "plt.plot(np.convolve(train_lossv, np.ones(101), 'same') / 101,\n",
        "         label='averaged')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig(\"training-loss.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YehHezB-J13b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference\n",
        "\n",
        "For a better measure of the quality of the model, let's see the\n",
        "model accuracy for the test reviews."
      ],
      "metadata": {
        "id": "cZ60h-XMMQcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nTest set:')\n",
        "evaluate(test_dataloader)\n",
        "# eof"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoW9yYglJ1vs",
        "outputId": "f9cb6e77-c960-403d-a5e2-9e612c18face"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set:\n",
            "Accuracy: [12741/13645] 0.9337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OOD detection "
      ],
      "metadata": {
        "id": "J5EOhawoaTit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract BERT logits, features and predictions"
      ],
      "metadata": {
        "id": "u_2e2L6da_xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_bert_features(loader):\n",
        "    model.eval()\n",
        "\n",
        "    last_layer_list = []\n",
        "    logits_list = []\n",
        "    softmax_list = []\n",
        "    label_list = []\n",
        "    pred_list = []\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "            last_layer = outputs[1][-1][:,0,:]\n",
        "            logits = outputs[0][:,:]\n",
        "            predictions = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "            softmax = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        last_layer_list.append(last_layer)\n",
        "        logits_list.append(logits.cpu().numpy())\n",
        "        softmax_list.append(softmax.cpu().numpy())\n",
        "        label_list.append(b_labels.cpu().numpy())\n",
        "        pred_list.append(predictions)\n",
        "    \n",
        "\n",
        "        \n",
        "\n",
        "    last_layer = torch.cat(last_layer_list, dim=0)\n",
        "    logits = np.concatenate(logits_list, axis=0)\n",
        "    softmax = np.concatenate(softmax_list, axis=0)\n",
        "    labels = np.concatenate(label_list, axis=0)\n",
        "    predictions = np.concatenate(pred_list, axis=0)\n",
        "\n",
        "    return logits, last_layer.to('cpu').numpy(),  softmax, labels, predictions\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "1VuC8taNaTHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimized memory version to be tested"
      ],
      "metadata": {
        "id": "UomewBAh1HtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "def extract_bert_features(loader):\n",
        "    model.eval()\n",
        "\n",
        "    last_layer_list = []\n",
        "    logits_list = []\n",
        "    softmax_list = []\n",
        "    label_list = []\n",
        "    pred_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "            last_layer = outputs[-1]\n",
        "            logits = outputs[0].detach().cpu().numpy()\n",
        "            predictions = np.argmax(logits, axis=1)\n",
        "            softmax = torch.nn.functional.softmax(outputs[0], dim=-1).cpu().numpy()\n",
        "\n",
        "            last_layer_list.append(last_layer.cpu())\n",
        "            logits_list.append(logits)\n",
        "            softmax_list.append(softmax)\n",
        "            label_list.append(b_labels.detach().cpu().numpy())\n",
        "            pred_list.append(predictions)\n",
        "\n",
        "    last_layer = torch.cat(last_layer_list, dim=0).numpy()\n",
        "    labels = np.concatenate(label_list, axis=0)\n",
        "    predictions = np.concatenate(pred_list, axis=0)\n",
        "\n",
        "    return last_layer.to('cpu').numpy(), logits_list, softmax_list, labels, predictions\n",
        "\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "PDIo_Jxxv03O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "63325905-1806-435e-b3c3-164c43639260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\ndef extract_bert_features(loader):\\n    model.eval()\\n\\n    last_layer_list = []\\n    logits_list = []\\n    softmax_list = []\\n    label_list = []\\n    pred_list = []\\n\\n    with torch.no_grad():\\n        for batch in loader:\\n            batch = tuple(t.to(device) for t in batch)\\n            b_input_ids, b_input_mask, b_labels = batch\\n\\n            outputs = model(b_input_ids, token_type_ids=None,\\n                            attention_mask=b_input_mask)\\n            last_layer = outputs[-1]\\n            logits = outputs[0].detach().cpu().numpy()\\n            predictions = np.argmax(logits, axis=1)\\n            softmax = torch.nn.functional.softmax(outputs[0], dim=-1).cpu().numpy()\\n\\n            last_layer_list.append(last_layer.cpu())\\n            logits_list.append(logits)\\n            softmax_list.append(softmax)\\n            label_list.append(b_labels.detach().cpu().numpy())\\n            pred_list.append(predictions)\\n\\n    last_layer = torch.cat(last_layer_list, dim=0).numpy()\\n    labels = np.concatenate(label_list, axis=0)\\n    predictions = np.concatenate(pred_list, axis=0)\\n\\n    return last_layer.to('cpu').numpy(), logits_list, softmax_list, labels, predictions\\n\\n    \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now applying on the data sets"
      ],
      "metadata": {
        "id": "2kS7LfTRMknK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = extract_bert_features(train_dataloader)\n",
        "test_features = extract_bert_features(test_dataloader)\n",
        "ood_features = extract_bert_features(ood_dataloader)"
      ],
      "metadata": {
        "id": "p0tBrqYkbDJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOD with Mahalanobis distance using last layer logits and features"
      ],
      "metadata": {
        "id": "DnC_LAB21srU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the function that calculates the metrics \n",
        "def metrics(scores: np.ndarray, labels: np.ndarray, threshold: float):\n",
        "    \"\"\"Computes the number of\n",
        "        * true positives,\n",
        "        * false positives,\n",
        "        * true negatives,\n",
        "        * false negatives,\n",
        "        * the accuracy\n",
        "        * false positive rate\n",
        "        * taux d'erreur\n",
        "    for a given threshold\n",
        "\n",
        "    \"\"\"\n",
        "    pos = np.where(scores >= threshold) \n",
        "    neg = np.where(scores < threshold)\n",
        "    n_pos = len(pos[0])\n",
        "    n_neg = len(neg[0])\n",
        "\n",
        "    tp = np.sum(labels[pos])\n",
        "    fp = n_pos - tp\n",
        "    fn = np.sum(labels[neg])\n",
        "    tn = n_neg - fn\n",
        "\n",
        "    FPR = fp / (fp + tn)\n",
        "    Accuracy = (tp+tn)/len(scores)\n",
        "    ERR = 1 - Accuracy\n",
        "    return FPR, ERR"
      ],
      "metadata": {
        "id": "mVLLg9QQBFpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(scores: np.ndarray, labels: np.ndarray, threshold: float):\n",
        "    \"\"\"Computes the number of\n",
        "        * true positives,\n",
        "        * false positives,\n",
        "        * true negatives,\n",
        "        * false negatives,\n",
        "        * the accuracy\n",
        "        * false positive rate\n",
        "        * taux d'erreur\n",
        "    for a given threshold\n",
        "\n",
        "    \"\"\"\n",
        "    pos = np.where(scores >= threshold) \n",
        "    neg = np.where(scores < threshold)\n",
        "    n_pos = len(pos[0])\n",
        "    n_neg = len(neg[0])\n",
        "\n",
        "    tp = np.sum(labels[pos])\n",
        "    fp = n_pos - tp\n",
        "    fn = np.sum(labels[neg])\n",
        "    tn = n_neg - fn\n",
        "\n",
        "    TPR = tp / (tp + fn)\n",
        "    FPR = fp / (fp + tn)\n",
        "    Accuracy = (tp+tn)/len(scores)\n",
        "    ERR = 1 - Accuracy\n",
        "    \n",
        "    # Calculate the new threshold for a TPR of 95%\n",
        "    new_TPR = 0.95\n",
        "    new_threshold = np.percentile(scores, (1 - new_TPR) * 100)\n",
        "    \n",
        "    # Calculate the new FPR value that is consistent with TPR = 95%\n",
        "    new_pos = np.where(scores >= new_threshold) \n",
        "    new_neg = np.where(scores < new_threshold)\n",
        "    new_n_pos = len(new_pos[0])\n",
        "    new_n_neg = len(new_neg[0])\n",
        "    new_tp = np.sum(labels[new_pos])\n",
        "    new_fp = new_n_pos - new_tp\n",
        "    new_fn = np.sum(labels[new_neg])\n",
        "    new_tn = new_n_neg - new_fn\n",
        "    new_FPR = new_fp / (new_fp + new_tn)\n",
        "\n",
        "    return new_FPR, FPR, ERR\n"
      ],
      "metadata": {
        "id": "pH7HotQSeUNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Covariance matrix for the latent features of the internal distribution (SST2)"
      ],
      "metadata": {
        "id": "Ainp69jYM75A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mean, Covariance matrix and inverse for the latent features of the internal distribution (SST2) \n",
        "                  #for logits#                          #for features#\n",
        "train_mean = [np.mean(train_features[0], axis=0)  ,np.mean(train_features[1], axis=0)]\n",
        "\n",
        "                  #for logits#                           #for features#\n",
        "train_cov = [np.cov(train_features[0], rowvar=False), np.cov(train_features[1], rowvar=False)] \n",
        "\n",
        "                   #for logits#                          #for features#\n",
        "train_inv_cov = [np.linalg.inv(train_cov[0]), np.linalg.inv(train_cov[1])] \n"
      ],
      "metadata": {
        "id": "je8mMk_GCyBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        " \n",
        "  #Calculation of the Mahalanobis distance on the in-ds and ood data\n",
        "  inds_test_scores = []\n",
        "  ood_test_scores = []\n",
        "\n",
        "  for feature in test_features[i]:\n",
        "    score = mahalanobis(feature, train_mean[i], train_inv_cov[i])\n",
        "    inds_test_scores.append(score)\n",
        "\n",
        "  for feature in ood_features[i]:\n",
        "    score = mahalanobis(feature, train_mean[i], train_inv_cov[i])\n",
        "    ood_test_scores.append(score)\n",
        "\n",
        "  #We will evaluate the performance of OOD detection using the AUROC,\n",
        "  # AUPR, FPR and ERR metrics\n",
        "  \n",
        "  labels = np.concatenate([np.zeros(len(inds_test_scores)), np.ones(len(ood_test_scores))])\n",
        "  scores = np.concatenate([inds_test_scores, ood_test_scores])\n",
        "\n",
        "  #define the threshold for ood detection\n",
        "  threshold = np.mean(inds_test_scores) +  np.std(inds_test_scores)\n",
        "\n",
        "  #Calculation of metrics\n",
        "  if i == 0:\n",
        "    use = 'logits'\n",
        "  else:\n",
        "      use = 'features'\n",
        "  new_FPR, FPR, ERR = metrics (scores, labels, threshold)\n",
        "  auroc = roc_auc_score(labels, scores)\n",
        "  aupr = average_precision_score(labels, scores)\n",
        "\n",
        "  \n",
        "  print(\"metrics in ood detection with Mahalanibos distance using \"+use+\" of last layer\")\n",
        "  print('AUROC:', auroc)\n",
        "  print('AUPR:', aupr)\n",
        "  print('FPR_95%:', new_FPR)\n",
        "  print('FPR:', FPR)\n",
        "  print('ERR:', ERR)\n",
        "\n",
        "\n",
        "\n",
        "  #plot of auroc curve and aupr curve\n",
        "  fpr, tpr, _ = roc_curve(labels, scores)\n",
        "  #precision, recall, _ = precision_recall_curve(labels, scores)\n",
        "  \n",
        "\n",
        "  plt.figure(figsize=(7, 7))\n",
        "  \n",
        "  plt.plot(fpr, tpr)\n",
        "  plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('AUROC Curve')\n",
        "\n",
        "\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(\"auroc_mahalabinos_\"+use+\".png\")\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfI--kWMBsKG",
        "outputId": "875e8283-80ae-407b-efae-bfa3a2a0f9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metrics in ood detection with Mahalanibos distance using logits of last layer\n",
            "AUROC: 0.7638375592837383\n",
            "AUPR: 0.6538371873357525\n",
            "FPR: 0.09050934408208135\n",
            "ERR: 0.20940215684221553\n",
            "metrics in ood detection with Mahalanibos distance using features of last layer\n",
            "AUROC: 0.8736117873621094\n",
            "AUPR: 0.741240498836859\n",
            "FPR: 0.15075119091242214\n",
            "ERR: 0.1871531776777301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOD detection using Energy-based score\n"
      ],
      "metadata": {
        "id": "5VNjfXwq2OJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the energy-based score function\n",
        "def energy_score(data:np.ndarray):\n",
        "  energy = np.log(np.sum(np.exp(data), axis=1))\n",
        "  \n",
        "  return -energy\n",
        "#we only use logits\n",
        "in_scores = energy_score(test_features[0])\n",
        "out_scores = energy_score(ood_features[0])\n",
        "\n",
        "labels = np.concatenate([np.zeros(len(in_scores)), np.ones(len(out_scores))])\n",
        "scores = np.concatenate([in_scores, out_scores])\n",
        "\n",
        "  #define the threshold for ood detection\n",
        "threshold = np.mean(in_scores) +  np.std(in_scores)"
      ],
      "metadata": {
        "id": "3BybTt0m_UKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_FPR,FPR, ERR = metrics (scores, labels, threshold)\n",
        "auroc = roc_auc_score(labels, scores)\n",
        "aupr = average_precision_score(labels, scores)\n",
        "\n",
        "  \n",
        "print(\"metrics in ood detection with Energy-based score using logits of last layer\")\n",
        "print('AUROC:', auroc)\n",
        "print('AUPR:', aupr)\n",
        "print('FPR_95%:', new_FPR)\n",
        "print('FPR:', FPR)\n",
        "print('ERR:', ERR)\n",
        "\n",
        "#plot of auroc curve and aupr curve\n",
        "fpr, tpr, _ = roc_curve(labels, scores)\n",
        "\n",
        "  \n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "  \n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('AUROC Curve')\n",
        "plt.savefig(\"auroc_energy.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4RMnXe6FXGx",
        "outputId": "55e3bbd8-e4b6-40ab-cbea-9f0dea31c6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metrics in ood detection with Energy-based score using logits of last layer\n",
            "AUROC: 0.7894638941729917\n",
            "AUPR: 0.5481632748531513\n",
            "FPR: 0.17222425796995236\n",
            "ERR: 0.2640037692388232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOD Detection using Maximum Softmax Probability"
      ],
      "metadata": {
        "id": "KiOq_iE8OqE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSP_score(data:np.ndarray):\n",
        "  \"inputs are softmax extract from BERT\"\n",
        "  score = np.max(data, axis=1)\n",
        "  return score\n",
        "\n",
        "#we calculate the scores for in and out data sets\n",
        "in_scores = energy_score(test_features[2])\n",
        "out_scores = energy_score(ood_features[2])\n",
        "\n",
        "labels = np.concatenate([np.zeros(len(in_scores)), np.ones(len(out_scores))])\n",
        "scores = np.concatenate([in_scores, out_scores])\n",
        "\n",
        "  #define the threshold for ood detection\n",
        "threshold = np.mean(in_scores) +  np.std(in_scores)\n"
      ],
      "metadata": {
        "id": "mWdFWbQVFYhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_FPR,FPR, ERR = metrics (scores, labels, threshold)\n",
        "auroc = roc_auc_score(labels, scores)\n",
        "aupr = average_precision_score(labels, scores)\n",
        "\n",
        "  \n",
        "print(\"metrics in ood detection with Energy-based score using logits of last layer\")\n",
        "print('AUROC:', auroc)\n",
        "print('AUPR:', aupr)\n",
        "print('FPR_95%:', new_FPR)\n",
        "print('FPR:', FPR)\n",
        "print('ERR:', ERR)\n",
        "\n",
        "#plot of auroc curve and aupr curve\n",
        "fpr, tpr, _ = roc_curve(labels, scores)\n",
        "\n",
        "  \n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "  \n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('AUROC Curve')\n",
        "plt.savefig(\"auroc_MSP.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XNTG0PmFm7J",
        "outputId": "1dcc7515-9aad-4e15-948d-babc02e0b484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metrics in ood detection with Energy-based score using logits of last layer\n",
            "AUROC: 0.7673714808060326\n",
            "AUPR: 0.49835768139054143\n",
            "FPR: 0.08933675338951998\n",
            "ERR: 0.2731651136006701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vk_-2eS5PvZ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}